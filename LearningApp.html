html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Reading Comprehension Test</title>
    <style>
        /* ----- General Enhancements ----- */
/* Modify body style */
body {
    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    line-height: 1.7;
    margin: 0; /* Remove auto margin here if using flex to center children */
    padding: 20px;
    /* max-width: 850px; <-- Keep this if you want the overall page to have a max-width */
    background-color: #e9ecef;
    color: #495057;
    display: flex;
    flex-direction: column;
    align-items: center; /* This will center flex items (our containers) */
    min-height: 100vh; /* Ensure body takes full viewport height */
}

#reading-app-container,
#questions-area,
#strategy-review-menu, /* Added menu container */
#sampling-visualization-container /* Added visualization container */
{
    background-color: #ffffff;
    padding: 25px 30px;
    border-radius: 12px;
    box-shadow: 0 5px 15px rgba(0,0,0,0.08);
    margin-bottom: 25px;
    width: 100%; /* Take full width of parent */
    max-width: 850px; /* Apply max-width to containers instead of body for this flex setup */
    /* margin-left: auto;  <-- Not needed if body is flex centering */
    /* margin-right: auto; <-- Not needed if body is flex centering */
}

        h1 {
            color: #343a40;
            margin-bottom: 25px;
            font-size: 2.2em;
        }
        h2 {
            color: #007bff; /* Make question headers stand out */
            margin-bottom: 20px;
            font-size: 1.8em;
        }

        button {
            padding: 12px 25px; /* Larger buttons */
            margin: 8px;
            cursor: pointer;
            background-color: #007bff;
            color: white;
            border: none;
            border-radius: 8px; /* More rounded buttons */
            font-size: 1em; /* Relative font size */
            font-weight: 500;
            transition: background-color 0.2s ease-in-out, transform 0.1s ease; /* Smooth transitions */
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        button:hover {
            background-color: #0056b3;
            transform: translateY(-1px); /* Slight lift on hover */
        }
        button:active {
            transform: translateY(0px); /* Press down effect */
        }
        button:disabled {
            background-color: #adb5bd; /* Clearer disabled state */
            cursor: not-allowed;
            box-shadow: none;
            transform: none;
        }

        #prev-page, #stay-on-page {
            background-color: #6c757d;
        }
        #prev-page:hover, #stay-on-page:hover {
            background-color: #545b62;
        }
        #proceed-to-questions {
            background-color: #28a745; /* Green for "proceed" */
        }
        #proceed-to-questions:hover {
            background-color: #1e7e34;
        }


        /* ----- Reading Area Specifics ----- */
        #text-container {
            border: 1px solid #dee2e6;
            padding: 20px;
            background-color: #f8f9fa; /* Slightly off-white for text background */
        }
        .page-info {
            color: #6c757d;
            font-size: 0.9em;
        }
        /* Style for the container holding the "Done Reading" and "View Visualization" buttons */
         #finish-reading-area {
             display: flex; /* Use flex to align buttons */
             justify-content: center; /* Center buttons horizontally */
             align-items: center;
             margin-top: 20px; /* Space above buttons */
         }
         #finish-reading-area button {
             margin: 0 10px; /* Space between buttons */
         }


        /* ----- Questions Area Specifics ----- */
        #question-text-container {
            margin-bottom: 25px;
            padding: 20px;
            border: 1px solid #ced4da;
            border-left: 5px solid #007bff; /* Accent border */
            background-color: #f8f9fa;
            border-radius: 8px;
        }
        #question-text {
            font-size: 1.15em; /* Larger question text */
            color: #343a40;
        }

        #options-container label {
            display: flex; /* For better alignment of radio and text */
            align-items: center;
            margin-bottom: 12px;
            padding: 15px; /* More padding in options */
            border: 1px solid #ced4da;
            border-radius: 8px;
            cursor: pointer;
            transition: background-color 0.2s ease, border-color 0.2s ease;
            background-color: #fff;
        }
        #options-container label:hover {
            background-color: #f1f3f5;
            border-color: #007bff;
        }
        #options-container input[type="radio"] {
            margin-right: 12px;
            flex-shrink: 0; /* Prevent radio from shrinking */
            /* Custom radio button appearance (optional advanced) */
            appearance: none;
            -webkit-appearance: none;
            width: 20px;
            height: 20px;
            border: 2px solid #adb5bd;
            border-radius: 50%;
            outline: none;
            cursor: pointer;
            transition: border-color 0.2s ease;
            position: relative; /* For the dot */
        }
        #options-container input[type="radio"]:checked {
            border-color: #007bff;
        }
        #options-container input[type="radio"]:checked::before {
            content: '';
            display: block;
            width: 10px;
            height: 10px;
            background-color: #007bff;
            border-radius: 50%;
            position: absolute;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
        }


        #feedback {
            margin-top: 25px;
            padding: 20px;
            border-radius: 8px;
            border-left-width: 5px;
            border-left-style: solid;
        }
        .correct-answer-feedback { /* For the feedback div */
            background-color: #d1e7dd;
            border-color: #28a745; /* Green border for correct feedback div */
        }
        .wrong-answer-feedback { /* For the feedback div */
            background-color: #dc3545;
            border-color: #dc3545; /* Red border for wrong feedback div */
        }

        #feedback-message {
            font-weight: 600; /* Bolder message */
            font-size: 1.1em;
            margin-bottom: 8px;
        }
        #explanation {
            font-size: 1em;
            color: #495057;
        }
        .correct-answer { /* Label highlighting */
            background-color: #d1e7dd !important;
            border-color: #28a745 !important;
            color: #0f5132 !important;
        }
        .wrong-answer { /* Label highlighting */
            background-color: #f8d7da !important;
            border-color: #dc3545 !important;
            color: #721c24 !important;
        }

        #quiz-summary {
            padding: 20px;
            border: 1px solid #ced4da;
            border-radius: 8px;
            background-color: #f8f9fa;
        }
        #quiz-summary h2 {
            color: #007bff;
        }
        #quiz-summary p {
            font-size: 1.1em;
            line-height: 1.6;
        }

        /* ----- Strategies Review Menu Styles ----- */
        #strategy-review-menu h2 {
            color: #007bff;
            margin-bottom: 20px;
        }
        #strategy-review-menu ul {
            list-style: none;
            padding: 0;
            margin: 0 0 20px 0; /* Space before back button */
        }
        #strategy-review-menu li {
            background-color: #e9ecef; /* Light background for list items */
            margin-bottom: 15px;
            padding: 15px;
            border-left: 4px solid #007bff; /* Accent border */
            border-radius: 4px;
            font-size: 1em;
        }
         #strategy-review-menu li strong {
             color: #343a40;
             font-size: 1.1em;
             display: block; /* Make strategy name a block for spacing */
             margin-bottom: 5px;
         }

        /* ----- Sampling Visualization Styles ----- */
        #sampling-visualization-container h2 {
             color: #007bff;
             margin-bottom: 20px;
         }
         .control-group {
             margin-bottom: 20px;
             padding: 15px;
             border: 1px solid #ced4da;
             border-radius: 8px;
             background-color: #f8f9fa;
         }
         .control-group label {
             display: block;
             margin-bottom: 10px;
             font-weight: bold;
             color: #343a40;
         }
          .control-group input[type="range"] {
             width: 80%; /* Adjust slider width */
             margin-right: 10px;
             vertical-align: middle; /* Align with number input */
         }
          .control-group input[type="number"] {
              width: 60px; /* Adjust number input width */
              padding: 5px;
              border: 1px solid #ced4da;
              border-radius: 4px;
              text-align: center;
              vertical-align: middle; /* Align with slider */
          }

         #sampling-explanation {
             margin-top: 25px;
             padding: 20px;
             border: 1px solid #dee2e6;
             border-left: 5px solid #28a745; /* Accent border */
             background-color: #e9ecef;
             border-radius: 8px;
             font-size: 1.1em;
             line-height: 1.6;
             color: #343a40;
         }

         #back-to-reading-vis {
             margin-top: 20px;
             background-color: #6c757d;
         }
          #back-to-reading-vis:hover {
             background-color: #545b62;
         }


        /* ----- Animations for Question Transition ----- */
        .question-transition-out {
            animation: fadeOut 0.3s forwards;
        }
        .question-transition-in {
            animation: fadeIn 0.4s forwards 0.1s; /* Slight delay for smoother feel */
        }

        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(20px); }
            to { opacity: 1; transform: translateY(0); }
        }
        @keyframes fadeOut {
            from { opacity: 1; transform: translateY(0); }
            to { opacity: 0; transform: translateY(-20px); }
        }
    </style>
</head>
<body>
    <div id="reading-app-container">
        <h1>Reading Material</h1>
        <!-- ADDED LINK HERE -->
        <p>Source Article: <a href="https://www.kaggle.com/whitepaper-prompt-engineering" target="_blank">Prompt Engineering Whitepaper</a></p>
        <!--------------------->
        <div id="text-container">
            <!-- Text content will be loaded here -->
        </div>
        <div class="page-info" id="page-indicator">Page 1 of N</div>
        <div class="navigation-buttons">
            <button id="prev-page">Previous Page</button>
            <button id="next-page">Next Page</button>
        </div>
         <!-- Consolidated finish reading/visualization buttons here -->
        <div id="finish-reading-area" style="display: none;">
            <button id="done-reading">I'm Done Reading</button>
            <button id="view-sampling-vis-button" style="display: none;">View Sampling Visualization</button>
        </div>
         <!------------------------------------------------------>
    </div>

    <div id="questions-area" style="display: none;">
        <h2>Question <span id="current-question-number">1</span> of <span id="total-questions">N</span></h2>
        <div id="question-text-container">
            <p id="question-text"></p>
        </div>
        <div id="options-container">
            <!-- Radio buttons and labels for options will be generated here by JS -->
        </div>
        <div id="feedback" style="display: none;">
            <p id="feedback-message"></p>
            <div id="explanation"></div>
            <img id="illustration" src="" alt="Illustration" style="display: none;">
        </div>
        <div id="quiz-summary" style="display:none; text-align:center;">
            <!-- Evaluation will be shown here -->
        </div>
        <button id="submit-answer">Submit Answer</button>
        <button id="next-question" style="display: none;">Next Question</button>
        <button id="back-to-reading" style="margin-top: 20px;">Back to Reading Material</button>
    </div>

     <!-- New Strategies Review Menu -->
     <div id="strategy-review-menu" style="display: none;">
         <h2>Prompting Strategies Summary</h2>
         <ul id="strategies-list">
             <!-- Strategy summaries will be populated here by JS -->
         </ul>
         <button id="back-to-summary">Back to Summary</button>
     </div>
     <!---------------------------->

     <!-- New Sampling Visualization -->
     <div id="sampling-visualization-container" style="display: none;">
         <h2>Sampling Control Visualization</h2>

         <div class="control-group">
             <label for="temp-slider">Temperature: <span id="temp-value">0.7</span></label>
             <input type="range" id="temp-slider" min="0" max="2" value="0.7" step="0.05">
             <input type="number" id="temp-number" min="0" max="2" value="0.7" step="0.05">
         </div>

         <div class="control-group">
             <label for="topk-slider">Top-K: <span id="topk-value">40</span></label>
             <input type="range" id="topk-slider" min="1" max="100" value="40" step="1">
             <input type="number" id="topk-number" min="1" max="100" value="40" step="1">
         </div>

         <div class="control-group">
             <label for="topp-slider">Top-P: <span id="topp-value">0.95</span></label>
             <input type="range" id="topp-slider" min="0" max="1" value="0.95" step="0.01">
             <input type="number" id="topp-number" min="0" max="1" value="0.95" step="0.01">
         </div>

         <div id="sampling-explanation">
             <!-- Explanation text will be generated here by JS -->
         </div>

         <!-- Optional: Add a simple conceptual visual here later if desired -->
         <!-- <div id="conceptual-visual"></div> -->


         <button id="back-to-reading-vis">Back to Reading Material</button>
     </div>
     <!--------------------------------->


    <div id="confirmation-popup">
        <p>Ready for Questions?</p>
        <button id="proceed-to-questions">Yes, Proceed</button>
        <button id="stay-on-page">No, Go Back</button>
    </div>

    <script>
        const pages = [
// Page 1: Introduction & Core Concepts
`Prompt Engineering

You don't need to be a data
scientist or a machine learning
engineer — everyone can write
a prompt.

Introduction

When thinking about a large language model input and output, a text prompt (sometimes
accompanied by other modalities such as image prompts) is the input the model uses
to predict a specific output. You don’t need to be a data scientist or a machine learning
engineer — everyone can write a prompt. However, crafting the most effective prompt can be
complicated. Many aspects of your prompt affect its efficacy: the model you use, the model’s
training data, the model configurations, your word-choice, style and tone, structure, and
context all matter. Therefore, prompt engineering is an iterative process. Inadequate prompts
can lead to ambiguous, inaccurate responses, and can hinder the model’s ability to provide
meaningful output.

September 2024 6

Prompt Engineering

When you chat with the Gemini chatbot,' you basically write prompts, however this
whitepaper focuses on writing prompts for the Gemini model within Vertex Al or by using
the API, because by prompting the model directly you will have access to the configuration
such as temperature etc.

This whitepaper discusses prompt engineering in detail. We will look into the various
prompting techniques to help you getting started and share tips and best practices to
become a prompting expert. We will also discuss some of the challenges you can face
while crafting prompts.

Prompt engineering

Remember how an LLM works; it’s a prediction engine. The model takes sequential text as
an input and then predicts what the following token should be, based on the data it was
trained on. The LLM is operationalized to do this over and over again, adding the previously
predicted token to the end of the sequential text for predicting the following token. The next
token prediction is based on the relationship between what’s in the previous tokens and what
the LLM has seen during its training.

When you write a prompt, you are attempting to set up the LLM to predict the right sequence
of tokens. Prompt engineering is the process of designing high-quality prompts that guide
LLMs to produce accurate outputs. This process involves tinkering to find the best prompt,
optimizing prompt length, and evaluating a prompt’s writing style and structure in relation
to the task. In the context of natural language processing and LLMs, a prompt is an input
provided to the model to generate a response or prediction.

September 2024`,

// Page 2: LLM Output Configuration
`Prompt Engineering

These prompts can be used to achieve various kinds of understanding and generation
tasks such as text summarization, information extraction, question and answering, text
classification, language or code translation, code generation, and code documentation
or reasoning.

Please feel free to refer to Google’s prompting guides” with simple and effective
prompting examples.

When prompt engineering, you will start by choosing a model. Prompts might need to be
optimized for your specific model, regardless of whether you use Gemini language models in
Vertex Al, GPT, Claude, or an open source model like Gemma or LLaMA.

Besides the prompt, you will also need to tinker with the various configurations of a LLM.

LLM output configuration

Once you choose your model you will need to figure out the model configuration. Most LLMs
come with various configuration options that control the LLM’s output. Effective prompt
engineering requires setting these configurations optimally for your task.

Output length
An important configuration setting is the number of tokens to generate in a response.
Generating more tokens requires more computation from the LLM, leading to higher energy
consumption, potentially slower response times, and higher costs.

September 2024

Prompt Engineering

Reducing the output length of the LLM doesn’t cause the LLM to become more stylistically
or textually succinct in the output it creates, it just causes the LLM to stop predicting more
tokens once the limit is reached. If your needs require a short output length, you'll also
possibly need to engineer your prompt to accommodate.

Output length restriction is especially important for some LLM prompting techniques, like
ReAct, where the LLM will keep emitting useless tokens after the response you want.

Sampling controls

LLMs do not formally predict a single token. Rather, LLMs predict probabilities for what the
next token could be, with each token in the LLM’s vocabulary getting a probability. Those
token probabilities are then sampled to determine what the next produced token will be.
Temperature, top-K, and top-P are the most common configuration settings that determine
how predicted token probabilities are processed to choose a single output token.

Temperature

Temperature controls the degree of randomness in token selection. Lower temperatures
are good for prompts that expect a more deterministic response, while higher temperatures
can lead to more diverse or unexpected results. A temperature of O (greedy decoding) is
deterministic: the highest probability token is always selected (though note that if two tokens
have the same highest predicted probability, depending on how tiebreaking is implemented
you may not always get the same output with temperature 0).

Temperatures close to the max tend to create more random output. And as temperature gets
higher and higher, all tokens become equally likely to be the next predicted token.

September 2024 9`,

// Page 3: LLM Output Config (Cont.) & Intro to Prompting Techniques
`Prompt Engineering

The Gemini temperature control can be understood in a similar way to the softmax function
used in machine learning. A low temperature setting mirrors a low softmax temperature (T),
emphasizing a single, preferred temperature with high certainty. A higher Gemini temperature
setting is like a high softmax temperature, making a wider range of temperatures around
the selected setting more acceptable. This increased uncertainty accommodates scenarios
where a rigid, precise temperature may not be essential like for example when experimenting
with creative outputs.

Top-K and top-P

Top-K and top-P (also known as nucleus sampling)* are two sampling settings used in LLMs
to restrict the predicted next token to come from tokens with the top predicted probabilities.
Like temperature, these sampling settings control the randomness and diversity of
generated text.

Top-K sampling selects the top K most likely tokens from the model’s predicted
distribution. The higher top-K, the more creative and varied the model’s output; the
lower top-K, the more restive and factual the model’s output. A top-K of 1 is equivalent to
greedy decoding.

Top-P sampling selects the top tokens whose cumulative probability does not exceed
a certain value (P). Values for P range from O (greedy decoding) to 1 (all tokens in the
LLM’s vocabulary).

The best way to choose between top-K and top-P is to experiment with both methods (or
both together) and see which one produces the results you are looking for.

September 2024 10

Prompt Engineering

Another important configuration setting is the number of tokens to generate in a response.
Be aware, generating more tokens requires more computation from the LLM, leading
to higher energy consumption and potentially slower response times, which leads to
higher costs.

Putting it all together

Choosing between top-K, top-P, temperature, and the number of tokens to generate,
depends on the specific application and desired outcome, and the settings all impact one
another. It’s also important to make sure you understand how your chosen model combines
the different sampling settings together.

lf temperature, top-K, and top-P are all available (as in Vertex Studio), tokens that meet
both the top-K and top-P criteria are candidates for the next predicted token, and then
temperature is applied to sample from the tokens that passed the top-K and top-P criteria. If
only top-K or top-P is available, the behavior is the same but only the one top-K or P setting
is used.

lf temperature is not available, whatever tokens meet the top-K and/or top-P criteria are then
randomly selected from to produce a single next predicted token.

At extreme settings of one sampling configuration value, that one sampling setting either
cancels out other configuration settings or becomes irrelevant.

If you set temperature to O, top-K and top-P become irrelevant-the most probable
token becomes the next token predicted. If you set temperature extremely high (above
1-generally into the 10s), temperature becomes irrelevant and whatever tokens make
it through the top-K and/or top-P criteria are then randomly sampled to choose a next
predicted token.

September 2024 11

Prompt Engineering

If you set top-K to 1, temperature and top-P become irrelevant. Only one token passes the
top-K criteria, and that token is the next predicted token. If you set top-K extremely high,
like to the size of the LLM’s vocabulary, any token with a nonzero probability of being the
next token will meet the top-K criteria and none are selected out.

If you set top-P to O (or a very small value), most LLM sampling implementations will then
only consider the most probable token to meet the top-P criteria, making temperature and
top-K irrelevant. If you set top-P to 1, any token with a nonzero probability of being the
next token will meet the top-P criteria, and none are selected out.

As a general starting point, a temperature of .2, top-P of .95, and top-K of 30 will give you
relatively coherent results that can be creative but not excessively so. If you want especially
creative results, try starting with a temperature of .9, top-P of .99, and top-K of 40. And if you
want less creative results, try starting with a temperature of .1, top-P of .9, and top-K of 20.
Finally, if your task always has a single correct answer (e.g., answering a math problem), start
with a temperature of O.

NOTE: With more freedom (higher temperature, top-K, top-P, and output tokens), the LLM
might generate text that is less relevant.

Prompting techniques

LLMs are tuned to follow instructions and are trained on large amounts of data so they can
understand a prompt and generate an answer. But LLMs aren’t perfect; the clearer your
prompt text, the better it is for the LLM to predict the next likely text. Additionally, specific
techniques that take advantage of how LLMs are trained and how LLMs work will help you get
the relevant results from LLMs

September 2024 12`,

// Page 4: Basic Prompting Techniques
`Prompt Engineering

Now that we understand what prompt engineering is and what it takes, let’s dive into some
examples of the most important prompting techniques.

General prompting / zero shot

A zero-shot® prompt is the simplest type of prompt. It only provides a description of a task
and some text for the LLM to get started with. This input could be anything: a question, a
start of a story, or instructions. The name zero-shot stands for ‘no examples’.

Let’s use Vertex Al Studio (for Language) in Vertex Al,° which provides a playground to test
prompts. In Table 1, you will see an example zero-shot prompt to classify movie reviews.

The table format as used below is a great way of documenting prompts. Your prompts will
likely go through many iterations before they end up in a codebase, so it’s important to keep
track of your prompt engineering work in a disciplined, structured way. More on this table
format, the importance of tracking prompt engineering work, and the prompt development
process is in the Best Practices section later in this chapter (“Document the various prompt
attempts”).

The model temperature should be set to a low number, since no creativity is needed, and we
use the gemini-pro default top-K and top-P values, which effectively disable both settings
(see ‘LLM Output Configuration’ above). Pay attention to the generated output. The words
disturbing and masterpiece should make the prediction a little more complicated, as both
words are used in the same sentence.
<pre>
September 2024

Prompt Engineering

Name 1.1 movie_classification
Goal Classify movie reviews as positive, neutral or negative.
Model gemini-pro
Temperature 0.1 Token Limit 5
Top-K N/A Top-P 1
Prompt Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.
Review: "Her" is a disturbing study revealing the direction
humanity is headed if AI is allowed to keep evolving,
unchecked. I wish there were more movies like this masterpiece.
Sentiment:
Output POSITIVE
Table 1. An example of zero-shot prompting
</pre>
When zero-shot doesn’t work, you can provide demonstrations or examples in the prompt,
which leads to “one-shot” and “few-shot” prompting. General prompting / zero shot

One-shot & few-shot

When creating prompts for Al models, it is helpful to provide examples. These examples can
help the model understand what you are asking for. Examples are especially useful when you
want to steer the model to a certain output structure or pattern.

A one-shot prompt, provides a single example, hence the name one-shot. The idea is the
model has an example it can imitate to best complete the task.

A few-shot prompt’ provides multiple examples to the model. This approach shows the
model a pattern that it needs to follow. The idea is similar to one-shot, but multiple examples
of the desired pattern increases the chance the model follows the pattern.

September 2024 14

Prompt Engineering

The number of examples you need for few-shot prompting depends on a few factors,
including the complexity of the task, the quality of the examples, and the capabilities of the
generative Al (gen Al) model you are using. As a general rule of thumb, you should use at
least three to five examples for few-shot prompting. However, you may need to use more
examples for more complex tasks, or you may need to use fewer due to the input length
limitation of your model.

Table 2 shows a few-shot prompt example, let’s use the same gemini-pro model
configuration settings as before, other than increasing the token limit to accommodate the
need for a longer response.
<pre>
Goal Parse pizza orders to JSON
Model gemini-pro
Temperature 0.1 Token Limit 250
Top-K N/A Top-P 1
Prompt Parse a customer's pizza order into valid JSON:
EXAMPLE:
I want a small pizza with cheese, tomato sauce, and pepperoni.
JSON Response:
{
"size": "small",
"type": "normal",
"ingredients": [["cheese", "tomato sauce", "peperoni"]]
}
Continues next page...

September 2024 15

Prompt Engineering

Prompt (Continued)
EXAMPLE:
Can I get a large pizza with tomato sauce, basil and mozzarella
JSON Response:
{
"size": "large",
"type": "normal",
"ingredients": [["tomato sauce", "bazel", "mozzarella"]]
}
Now, I would like a large pizza, with the first half cheese and
mozzarella. And the other tomato sauce, ham and pineapple.
JSON Response:
Output (Continued)
{
"size": "large",
"type": "half-half",
"ingredients": [["cheese", "mozzarella"], ["tomato sauce", "ham", "pineapple"]]
}
Table 2. An example of few-shot prompting
</pre>
When you choose examples for your prompt, use examples that are relevant to the task you
want to perform. The examples should be diverse, of high quality, and well written. One small
mistake can confuse the model and will result in undesired output.

If you are trying to generate output that is robust to a variety of inputs, then it is important to
include edge cases in your examples. Edge cases are inputs that are unusual or unexpected,
but that the model should still be able to handle.

September 2024`,

// Page 5: Advanced Prompting Techniques - Part 1 (System, Contextual, Role)
`Prompt Engineering

System, contextual and role prompting
System, contextual and role prompting are all techniques used to guide how LLMs generate
text, but they focus on different aspects:

System prompting sets the overall context and purpose for the language model. It
defines the ‘big picture’ of what the model should be doing, like translating a language,
classifying a review etc.

Contextual prompting provides specific details or background information relevant to
the current conversation or task. It helps the model to understand the nuances of what’s
being asked and tailor the response accordingly.

Role prompting assigns a specific character or identity for the language model to adopt.
This helps the model generate responses that are consistent with the assigned role and its
associated knowledge and behavior.

There can be considerable overlap between system, contextual, and role prompting. E.g. a
prompt that assigns a role to the system, can also have a context.

However, each type of prompt serves a slightly different primary purpose:

System prompt: Defines the model’s fundamental capabilities and overarching purpose.
Contextual prompt: Provides immediate, task-specific information to guide the response.
It’s highly specific to the current task or input, which is dynamic.
Role prompt: Frames the model’s output style and voice. It adds a layer of specificity
and personality.

September 2024 17

Prompt Engineering

Distinguishing between system, contextual, and role prompts provides a framework for
designing prompts with clear intent, allowing for flexible combinations and making it easier to
analyze how each prompt type influences the language model’s output.

Let’s dive into these three different kinds of prompts.

System prompting

Table 3 contains a system prompt, where | specify additional information on how to return the
output. | increased the temperature to get a higher creativity level, and | specified a higher
token limit. However, because of my clear instruction on how to return the output the model
didn’t return extra text.
<pre>
Goal Classify movie reviews as positive, neutral or negative.
Model gemini-pro
Temperature 1 Token Limit 5
Top-K 40 Top-P 0.8
Prompt Classify movie reviews as positive, neutral or negative. Only
return the label in uppercase.
Review: "Her" is a disturbing study revealing the direction
humanity is headed if AI is allowed to keep evolving,
unchecked. It's so disturbing I couldn't watch it.
Sentiment:
Output NEGATIVE
Table 3. An example of system prompting
</pre>
September 2024 18

Prompt Engineering

System prompts can be useful for generating output that meets specific requirements. The
name ‘system prompt’ actually stands for ‘providing an additional task to the system’. For
example, you could use a system prompt to generate a code snippet that is compatible with
a specific programming language, or you could use a system prompt to return a certain
structure. Have a look into Table 4, where | return the output in JSON format.

September 2024
<pre>
Prompt Engineering

Goal Classify movie reviews as positive, neutral or negative, return JSON.
Model gemini-pro
Temperature 1 Token Limit 1024
Top-K 40 Top-P 0.8
Prompt Classify movie reviews as positive, neutral or negative. Return
valid JSON:
Review: "Her" is a disturbing study revealing the direction
humanity is headed if AI is allowed to keep evolving,
unchecked. It's so disturbing I couldn't watch it.
Schema:
MOVIE:
{
"sentiment": String "POSITIVE" | "NEGATIVE" | "NEUTRAL",
"name": String
}
MOVIE REVIEWS:
{
"movie_reviews": [MOVIE]
}
JSON Response:
Output {
"movie_reviews': [
{
"sentiment": "NEGATIVE",
"name": "Her"
}
]
}
Table 4. An example of system prompting with JSON format
</pre>
September 2024 20

Prompt Engineering

There are some benefits in returning JSON objects from a prompt that extracts data. In
a real-world application | don’t need to manually create this JSON format, | can already
return the data in a sorted order (very handy when working with datetime objects), but most
importantly, by prompting for a JSON format it forces the model to create a structure and
limit hallucinations.

System prompts can also be really useful for safety and toxicity. To control the output, simply
add an additional line to your prompt like: ‘You should be respectful in your answer.’`,

// Page 6: Role & Contextual Prompting
`Role prompting

Role prompting is a technique in prompt engineering that involves assigning a specific role to
the gen Al model. This can help the model to generate more relevant and informative output,
as the model can craft its responses to the specific role that it has been assigned.

For example, you could role prompt a gen Al model to be a book editor, a kindergarten
teacher, or a motivational speaker. Once the model has been assigned a role, you can then
give it prompts that are specific to that role. For example, you could prompt a teacher to
create a lesson plan that they can then review. Have a look into Table 5, where the model
takes the role of a travel guide.

September 2024 21
<pre>
Prompt Engineering

Goal Act as travel guide and provide 3 travel suggestions
Model gemini-pro
Temperature 1 Token Limit 1024
Top-K 40 Top-P 0.8
Prompt I want you to act as a travel guide. I will write to you
about my location and you will suggest 3 places to visit near
me. In some cases, I will also give you the type of places I
will visit.
My suggestion: "I am in Amsterdam and I want to visit
only museums."
Output Travel Suggestions:
Rijksmuseum: World-renowned museum, known for Dutch masterpieces
like Rembrandt’s “The Night Watch.” Don’t miss the incredible collection of
paintings and artifacts showcasing Dutch art and history.
Van Gogh Museum: Immerse yourself in the captivating works of Vincent
van Gogh. The museum houses the largest collection of his paintings and
drawings, including “The Starry Night” and “Sunflowers.”
Stedelijk Museum Amsterdam: Discover modern and contemporary art
from around the world. The museum’s collection includes works by Picasso,
Kandinsky, and Rietveld, housed in a striking modern building.
Table 5. An example of role prompting
</pre>
Defining a role perspective for an Al model gives it a blueprint of the tone, style, and focused
expertise you’re looking for to improve the quality, relevance, and effectiveness of your
output.

Here are some styles you can choose from which | find effective:
Confrontational, Descriptive, Direct, Formal, Humorous, Influential, Informal,
Inspirational, Persuasive

September 2024 22

Prompt Engineering

Let’s change our prompt in Table 6 to include a humorous and inspirational style.
<pre>
Prompt I want you to act as a travel guide. I will write to you about
my location and you will suggest 3 places to visit near me in
a humorous style.
My suggestion: "I am in Manhattan."
Output Travel Suggestions:
1. Behold the Empire State of Mind: Ascend to the dizzying heights of the Empire
State Building and bask in the glory of Manhattan's skyline. Prepare to feel like
King Kong atop the Big Apple, minus the giant ape-sized banana.
2. Get Artsy-Fartsy at MoMA: Unleash your inner art aficionado at the Museum
of Modern Art (MoMA). Gaze upon masterpieces that will boggle your
mind and make you question whether your stick-figure drawings have any
artistic merit.
3. Shop ‘Til You Drop on Fifth Avenue: Indulge in some retail therapy on the
iconic Fifth Avenue. Brace yourself for sticker shock as you window-shop (or
actually shop) at designer boutiques that will make your wallet cry. But hey,
you’re in Manhattan, so you might as well embrace the fabulousness!
Table 6. An example of role prompting with a humorous tone and style
</pre>
Contextual prompting

By providing contextual prompts, you can help ensure that your Al interactions are as
seamless and efficient as possible. The model will be able to more quickly understand your
request and be able to generate more accurate and relevant responses, as you can see in the
example of Table 7.

September 2024 23
<pre>
Prompt Engineering

Goal Suggest articles for a blog about retro games
Model gemini-pro
Temperature 1 Token Limit 1024
Top-K 40 Top-P 0.8
Prompt Context: You are writing for a blog about retro 80's arcade
video games.
Suggest 3 topics to write an article about with a few lines of
description of what this article should contain.
Output 1. The Evolution of Arcade Cabinet Design:
This article would explore the evolution of arcade cabinet designs, from the
early wood and metal cabinets of the 1970s to the sleek, neon-lit designs of
the 1980s and beyond.
2. Blast From The Past: Iconic Arcade Games of The 80’s:
This article would feature a glimpse into the history, impact, and legacy
of some of the most iconic and influential arcade games of the 1980s,
highlighting their innovations, popular mechanics, and enduring charm.
3. The Rise and Retro Revival of Pixel Art:
This article would delve into the evolution of pixel art as a defining visual
style of early arcade games, tracing its roots, exploring its techniques, and
discussing the resurgence of pixel art in modern games and digital art.
Table 7. An example of contextual prompting
</pre>
September 2024`,

// Page 7: Step-back & Chain of Thought (Intro)
`Prompt Engineering

Step-back prompting

Step-back® prompting is a technique for improving the performance by prompting the LLM
to first consider a general question related to the specific task at hand, and then feeding the
answer to that general question into a subsequent prompt for the specific task. This ‘step
back’ allows the LLM to activate relevant background knowledge and reasoning processes
before attempting to solve the specific problem.

By considering the broader and underlying principles, LLMs can generate more accurate and
insightful responses. Step-back prompting encourages LLMs to think critically and apply their
knowledge in new and creative ways. It changes the final prompt doing the task by utilizing
more knowledge in the LLM’s parameters than would otherwise come into play when the LLM
is prompted directly.

It can help to mitigate biases in LLM responses, by focusing on general principles instead of
specific details, step-back prompting.

Let’s have a look into these examples to understand how step-back prompting can improve
the results. Let’s first review a traditional prompt (Table 8) before we compare it to a step
back prompt (Table 9):

September 2024 25
<pre>
Prompt Engineering

Goal Write a storyline for a level of a first-person shooter video game.
Model gemini-pro
Temperature 1 Token Limit 1024
Top-K 40 Top-P 0.8
Prompt Write a one paragraph storyline for a new level of a first-
person shooter video game that is challenging and engaging.
Output The level begins with the player’s squad being ambushed in a dense urban area
by arelentless enemy faction. Players must fight their way through a maze of
back alleys and dilapidated buildings, utilizing stealth and precision to eliminate
enemy forces and avoid detection. The player must use their environment to their
advantage, setting traps, utilizing cover, and making calculated moves to survive.
Along the way, they uncover intel suggesting the enemy is planning a devastating
attack, and the player must race against time to gather evidence and relay it back
to their headquarters before the enemy can carry out their plans.
Table 8. A traditional prompt before we compare it with a step back prompt
</pre>
When you set the temperature to 1, you might end up with all kinds of creative writings for a
storyline, but it’s also quite random and generic. So let’s take a step back:

September 2024 26
<pre>
Prompt Engineering

Prompt Based on popular first-person shooter action games, what are
5 fictional key settings that contribute to a challenging and
engaging level storyline in a first-person shooter video game?
Output 1. Abandoned Military Base: A sprawling, post-apocalyptic military complex
crawling with mutated soldiers and rogue robots, ideal for challenging
firearm combat.
2. Cyberpunk City: A neon-lit, futuristic urban environment with towering
skyscrapers and dense alleyways, featuring cybernetically enhanced enemies
and hacking mechanics.
3. Alien Spaceship: A vast alien vessel stranded on Earth, with eerie
corridors, zero-gravity sections, and extraterrestrial creatures to encounter.
4. Zombie-Infested Town: A desolate town overrun by hordes of aggressive
zombies, featuring intense close-quarters combat and puzzle-solving to find
safe passage.
5. Underwater Research Facility: A deep-sea laboratory flooded with water,
filled with mutated aquatic creatures, and requiring stealth and underwater
exploration skills to survive.
Table 9. An example of prompting for self consistency
</pre>
Yeah those topics seem like a good fit for a first-person video game. Let’s go back to the
original prompt, but this time we include the answer of the step back as context and see what
it will return.

September 2024 27
<pre>
Prompt Engineering

Prompt Context: 5 engaging themes for a first person shooter video game:
1. Abandoned Military Base: A sprawling, post-apocalyptic
military complex crawling with mutated soldiers and rogue
robots, ideal for challenging firearm combat.
2. Cyberpunk City: A neon-lit, futuristic urban environment
with towering skyscrapers and dense alleyways, featuring
cybernetically enhanced enemies and hacking mechanics.
3. Alien Spaceship: A vast alien vessel stranded on
Earth, with eerie corridors, zero-gravity sections, and
extraterrestrial creatures to encounter.
4. Zombie-Infested Town: A desolate town overrun by hordes of
aggressive zombies, featuring intense close-quarters combat and
puzzle-solving to find safe passage.
5. Underwater Research Facility: A deep-sea laboratory flooded
with water, filled with mutated aquatic creatures, and requiring
stealth and underwater exploration skills to survive.
Take one of the themes and write a one paragraph storyline
for a new level of a first-person shooter video game that is
challenging and engaging.
Output In the heart of a murky abyss, lies a dilapidated underwater research facility,
standing as a testament to human ambition and its disastrous consequences.
Shrouded in darkness, pulsating with the hum of malfunctioning machinery, and
haunted by the echoes of drowned souls, the facility is now a breeding ground for
a nightmare borne from the deep sea. The player, an elite marine equipped with
advanced diving gear and experimental weaponry, is tasked with venturing into
this aquatic realm of terror. Armed with resilience and resourcefulness, they must
navigate treacherous corridors, solve cryptic puzzles, and confront gruesome sea
monstrosities that lurk in the depths. From grappling with the crushing pressure
of the deep sea to outsmarting cunning aquatic predators, every moment in this
uncharted underworld tests the limits of human endurance and courage.
Table 10. An example of prompting for self consistency
</pre>
That looks like an interesting video game! By using step back prompting techniques you can
increase the accuracy of your prompts.

September 2024 28`,

// Page 8: Chain of Thought (CoT)
`Prompt Engineering

Chain of Thought (CoT)

Chain of Thought (CoT) ° prompting is a technique for improving the reasoning capabilities
of LLMs by generating intermediate reasoning steps. This helps the LLM generate more
accurate answers. You can combine it with few-shot prompting to get better results on more
complex tasks that require reasoning before responding as it’s a challenge with a zero-shot
chain of thought.

CoT has a lot of advantages. First of all, it’s low-effort while being very effective and works
well with off-the-shelf LLMs (so no need to finetune). You also get interpretability with CoT
prompting, as you can learn from the LLM’s responses and see the reasoning steps that were
followed. If there’s a malfunction, you will be able to identify it. Chain of thought appears
to improve robustness when moving between different LLM versions. Which means the
performance of your prompt should drift less between different LLMs than if your prompt
does not use reasoning chains. Of course there are also disadvantages, but they are
somewhat intuitive.

The LLM response includes the chain of thought reasoning, which means more output
tokens, which means predictions cost more money and take longer.

To explain the following example in Table 11, let’s first try to create a prompt that is not using
CoT prompting to showcase the flaws of a large language model.
<pre>
Prompt When I was 3 years old, my partner was 3 times my age. Now, I
am 20 years old. How old is my partner?
Output 63 years old
Table 11. An example of a prompt which is trying to solve a mathematical problem
</pre>
September 2024 29

Prompt Engineering

Yikes. That’s obviously the wrong answer. As a matter of fact, LLMs often struggle with
mathematical tasks and can provide incorrect answers — even for a task as simple as
multiplying two numbers. This is because they are trained on large volumes of text and math
may require a different approach. So let’s see if intermediate reasoning steps will improve
the output.
<pre>
Prompt When I was 3 years old, my partner was 3 times my age. Now,
I am 20 years old. How old is my partner? Let's think step
by step.
A:
Output 1. Let’s assume my current age is “x” years.
When I was 3 years old, my age was 3 years.
My partner’s age at that time was 3 times my age, which means my partner’s
age was 3 * 3 = 9 years.
Now, I am 20 years old, which means my age has increased by 20 - 3 = 17
years since I was 3 years old.
Since my partner’s age was 9 years when I was 3 years old, and my age has
increased by 17 years, my partner’s age must have also increased by 17 years.
Therefore, my partner’s current age is 9 + 17 = 26 years.
So, my partner is 26 years old.
Table 12. An example of Chain of Thought prompting
</pre>
Alright, now the final answer is correct. That’s because we clearly instructed the LLM to
explain each step instead of just returning an answer. It’s interesting to see that the model
adds up the 17 increased years. In my head | would take the difference of years between my
partner and me and add those up. (20+(9-3)). Let’s help the model to think a little bit more
like me.

September 2024 30

Prompt Engineering

Table 12 is an example of ‘zero-shot’ Chain of thought. Chain of thought prompting can be
very powerful when combined with a single-shot or few-shot, as what you can see in Table 13:
<pre>
Prompt Q: When my brother was 2 years old, I was double his age. Now
I am 40 years old. How old is my brother? Let's think step
by step.
A: When my brother was 2 years, I was 2 * 2 = 4 years old.
That's an age difference of 2 years and I am older. Now I am 40
years old, so my brother is 40 - 2 = 38 years old. The answer
is 38.

Q: When I was 3 years old, my partner was 3 times my age. Now,
I am 20 years old. How old is my partner? Let's think step
by step.
A:
Output When I was 3 years old, my partner was 3 * 3 = 9 years old. That’s an age
difference of 6 years and my partner is older. Now I am 20 years old, so my
partner is 20 + 6 = 26 years old. The answer is 26.
Table 13. An example of chain of thought prompting with a single-shot
</pre>
Chain of thought can be useful for various use-cases. Think of code generation, for breaking
down the request into a few steps, and mapping those to specific lines of code. Or for
creating synthetic data when you have some kind of seed like “The product is called XYZ,
write a description guiding the model through the assumptions you would make based on
the product given title.” Generally, any task that can be solved by ‘talking through is a good
candidate for a chain of thought. If you can explain the steps to solve the problem, try chain
of thought.

Please refer to the notebook” hosted in the GoogleCloudPlatform Github repository which
will go into further detail on CoT prompting:

September 2024 31`,

// Page 9: Self-consistency & Tree of Thoughts (ToT)
`Prompt Engineering

In the best practices section of this chapter, we will learn some best practices specific to
Chain of thought prompting.

Self-consistency

While large language models have shown impressive success in various NLP tasks, their
ability to reason is often seen as a limitation that cannot be overcome solely by increasing
model size. As we learned in the previous Chain of Thought prompting section, the model can
be prompted to generate reasoning steps like a human solving a problem. However CoT uses
a simple ‘greedy decoding’ strategy, limiting its effectiveness. Self-consistency" combines
sampling and majority voting to generate diverse reasoning paths and select the most
consistent answer. It improves the accuracy and coherence of responses generated by LLMs.

Self-consistency gives a pseudo-probability likelihood of an answer being correct, but
obviously has high costs.
It follows the following steps:

1. Generating diverse reasoning paths: The LLM is provided with the same prompt multiple
times. A high temperature setting encourages the model to generate different reasoning
paths and perspectives on the problem.
2. Extract the answer from each generated response.
3. Choose the most common answer.

Let’s look into an example of an email classification system, which classifies an email as
IMPORTANT or NOT IMPORTANT. A zero-shot chain of thought prompt will be sent to the LLM
multiple times, to see if the responses differ after each submit. Notice the friendly tone, the
word choice and the sarcasm that’s been used in the email. All this could trick the LLM.

September 2024 32
<pre>
Prompt Engineering

Prompt
EMAIL:
Hi,
I have seen you use Wordpress for your website. A great open
source content management system. I have used it in the past
too. It comes with lots of great user plugins. And it's pretty
easy to set up.
I did notice a bug in the contact form, which happens when
you select the name field. See the attached screenshot of me
entering text in the name field. Notice the JavaScript alert
box that I inv@k3d.
But for the rest it's a great website. I enjoy reading it. Feel
free to leave the bug in the website, because it gives me more
interesting things to read.
Cheers,
Harry the Hacker.
Classify the above email as IMPORTANT or NOT IMPORTANT. Let's
think step by step and explain why.
Continues next page...

September 2024 33

Prompt Engineering

Output Step 1: Identify the purpose of the email.
Attempt 1 The purpose of the email is to inform the recipient of a bug in the contact form
on their website. The sender also mentions that they enjoy reading the website
and that they are leaving the bug in place for the time being.
Step 2: Assess the potential impact of the bug.
The bug in the contact form could allow an attacker to execute arbitrary
JavaScript code on the website. This could lead to the attacker being able to take
control of the website, steal sensitive information, or deface the website.
Step 3: Consider the credibility of the sender.
The sender of the email claims to be a hacker named Harry. However, there is no
way to verify the sender’s identity. It is possible that the sender is a legitimate
hacker who is trying to help the recipient, but it is also possible that the sender
is a malicious actor who is trying to trick the recipient into clicking on a malicious
link or downloading a malicious file.
Conclusion: IMPORTANT
Based on the potential impact of the bug and the credibility of the sender,
the email should be classified as IMPORTANT. The recipient should take
immediate action to fix the bug and protect their website from attack.

Output Explanation:
Attempt 2
1. Lack of urgency: The email does not convey any sense of urgency or
time-sensitivity. It starts with a casual greeting and presents the information
in a matter-of-fact tone.
2. Non-critical bug report: The bug reported in the contact form is
described as interesting but not critical. The email sender even suggests that
the website owner can leave the bug in place.
3. Lack of personal impact: The email does not indicate any direct or
personal impact on the website owner. It simply mentions that the sender
enjoy reading the website.
Continues next page...

September 2024 34

Prompt Engineering

Output (Attempt 2 Continued)
4. Absence of action request: The email does not explicitly request any
action from the website owner. It does not ask for the bug to be fixed or
provide any specific suggestions for improvement.
5. Sender’s intent: The sender’s intention seems to be more focused on
sharing an observation and expressing interest in the website rather than
raising a serious concern or requesting assistance.
**Conclusion: NOT IMPORTANT **

Output Step 1: Identify the purpose of the email.
Attempt 3 The purpose of the email is to inform the recipient of a bug in the contact form on
their website. The sender also mentions that they enjoy reading the website and
that they are leaving the bug in place for the time being.
Step 2: Assess the potential impact of the bug.
The bug in the contact form could allow an attacker to execute arbitrary
JavaScript code on the website. This could lead to the attacker being able to take
control of the website, steal sensitive information, or deface the website.
Step 3: Consider the credibility of the sender.
The sender of the email claims to be a hacker named Harry. However, there is no
way to verify the sender’s identity. It is possible that the sender is a legitimate
hacker who is trying to help the recipient, but it is also possible that the sender is
a malicious actor who is trying to exploit the bug for their own gain.
Conclusion:
Based on the above factors, the email should be classified as IMPORTANT.
The bug in the contact form is a serious security risk, and the sender’s credibility
is unknown. The recipient should take immediate action to fix the bug and
Table 14. An example of prompting for self consistency
</pre>
You can use the above prompt and try to see if it returns a consistent classification.
Depending on the model that you are using and the temperature configuration, it might
return “IMPORTANT” or “NOT IMPORTANT”.

September 2024 35

Prompt Engineering

By generating many Chains of Thoughts, and taking the most commonly occurring answer
(“IMPORTANT”), we can get a more consistently correct answer from the LLM.
This example shows how self-consistency prompting can be used to improve the accuracy
of an LLM’s response by considering multiple perspectives and selecting the most
consistent answer.

Tree of Thoughts (ToT)
Now that we are familiar with chain of thought and self-consistency prompting, let’s review
Tree of Thoughts (ToT)."? It generalizes the concept of CoT prompting because it allows LLMs
to explore multiple different reasoning paths simultaneously, rather than just following a
single linear chain of thought. This is depicted in Figure 1.
<pre>
| v
September 2024 36

Prompt Engineering

Figure 1. A visualization of chain of thought prompting on the left versus. Tree of Thoughts prompting on
the right
</pre>
This approach makes ToT particularly well-suited for complex tasks that require exploration. It
works by maintaining a tree of thoughts, where each thought represents a coherent language
sequence that serves as an intermediate step toward solving a problem. The model can then
explore different reasoning paths by branching out from different nodes in the tree.
There’s a great notebook, which goes into a bit more detail showing The Tree of Thought
(ToT) which is based on the paper ‘Large Language Model Guided Tree-of-Thought’?

September 2024 37`,

// Page 10: ReAct & Automatic Prompt Engineering
`Prompt Engineering

ReAct (reason & act)

Reason and act (ReAct) [10]"* prompting is a paradigm for enabling LLMs to solve complex
tasks using natural language reasoning combined with external tools (search, code
interpreter etc.) allowing the LLM to perform certain actions, such as interacting with external
API to retrieve information which is a first step towards agent modeling.

ReAct mimics how humans operate in the real world, as we reason verbally and can
take actions to gain information. ReAct performs well against other prompt engineering
approaches in a variety of domains.

ReAct prompting works by combining reasoning and acting into a thought-action loop. The
LLM first reasons about the problem and generates a plan of action. It then performs the
actions in the plan and observes the results. The LLM then uses the observations to update
its reasoning and generate a new plan of action. This process continues until the LLM
reaches a solution to the problem.

September 2024 37

Prompt Engineering

To see this in action, you need to write some code. In code Snippet 1! am using the langchain
framework for Python, together with VertexAl (google-cloud-aiplatform) and the
google-search-results pip packages.
To run this sample you must create a (free) SerpAPI key from https://serpapi.com/manage-
api-key and set an environment variable SERPAPI_API_KEY.
Next let’s write some Python code, with the task for the LLM to figure out: How many children
have a famous dad that performs in the band Metallica.
<pre><code>
Python
from langchain.agents import load_tools
from langchain.agents import initialize_agent
from langchain.agents import AgentType
from langchain.llms import VertexAI

prompt = "How many kids do the band members of Metallica have?"
llm = VertexAI(temperature=0.1)
tools = load_tools(["serpapi"], llm=llm)
agent = initialize_agent(tools, llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)
agent.run(prompt)
Snippet 1. Creating a ReAct Agent with LangChain and VertexAl
</code></pre>
Code Snippet 2 shows the result. Notice that ReAct makes a chain of five searches. In fact,
the LLM is scraping Google search results to figure out the band names. Then, it lists the
results as observations and chains the thought for the next search.

September 2024 38
<pre><code>
Prompt Engineering
Unset
Entering new AgentExecutor chain...
Metallica has 4 members.
Action: Search
Action Input: How many kids does James Hetfield have?
Observation: three children
Thought: 1/4 Metallica band members have 3 children
Action: Search
Action Input: How many kids does Lars Ulrich have?
Observation: 3
Thought: 2/4 Metallica band members have 6 children
Action: Search
Action Input: How many kids does Kirk Hammett have?
Observation: Hammett has been married to his second wife Lani since 1998.
They have two sons, Angel (b. September 29, 2006) and Vincenzo (b. June
28, 2008).
Thought: 3/4 Metallica band members have 8 children
Action: Search
Action Input: How many kids does Robert Trujillo have?
Observation: 2
Thought: 4/4 Metallica band members have 10 children
Final Answer: 10
Snippet 2. Executing ReAct Agent in the terminal
</code></pre>
Code Snippet 2 figures out that the band Metallica has four band members. Then it searches
each band member to request the total of children and add up the total. Finally, it returns the
total number of children as the final answer.

September 2024 39

Prompt Engineering

ReAct prompting in practice requires understanding that you continually have to resend the
previous prompts/responses (and do trimming of the extra generated content) as well as set
up the model with appropriate examples/instructions. Please refer to the notebook” hosted
in the GoogleCloudPlatform Github repository, which goes into a bit more detail showing the
actual LLM inputs and outputs with a more elaborate example

Automatic Prompt Engineering

At this point you might realize that writing a prompt can be complex. Wouldn’t it be nice to
automate this (write a prompt to write prompts)? Well, there’s a method: Automatic Prompt
Engineering (APE). This method" not only alleviates the need for human input but also
enhances the model’s performance in various tasks.
You will prompt a model to generate more prompts. Evaluate them, possibly alter the good
ones. And repeat.
For example, you can use automatic prompt engineering that will help with training a chatbot
for a merchandise t-shirt webshop. We want to figure out all the various ways customers
could phrase their order for buying a band merchandise t-shirt.
Write the prompt which will generate the output variants. In this example, | am using
gemini-pro to generate 10 instructions. See Table 15:

September 2024 40
<pre>
Prompt Engineering

Prompt We have a band merchandise t-shirt webshop, and to train a
chatbot we need various ways to order: "One Metallica t-shirt
size S". Generate 10 variants, with the same semantics but keep
the same meaning.
Output —
1. I’d like to purchase a Metallica t-shirt in size small.
2. Can I order a small-sized Metallica t-shirt?
3. I’d like to buy one Metallica t-shirt in small.
4. One Metallica shirt, size small, please.
5. I want one Metallica t-shirt in size small.
6. Id like to place an order for one Metallica t-shirt in small.
7. Small Metallica t-shirt, one please.
8. I’m looking for one Metallica t-shirt in small.
9. Can you get me one Metallica t-shirt in small?
10. One Metallica small t-shirt, please.
Table 15. Use an LLM to generate prompts
</pre>
Evaluate all of the instruction candidates by scoring the candidates based on a chosen
metric, For example, you can use. BLEU (Bilingual Evaluation Understudy) or ROUGE
(Recall-Oriented Understudy for Gisting Evaluation).
Select the instruction candidate with the highest evaluation score. This candidate will be
the final prompt you can use in your software application or chatbot. You can also tweak
the select prompt and evaluate again.

September 2024 A
`,

// Page 11: Code Prompting (Writing, Explaining, Translating)
`Prompt Engineering

Code prompting

Gemini focuses primarily on text-based prompts, which also includes writing prompts for
returning code. Let’s go to the Vertex Al Studio and test these prompts to look at some
coding examples.

Prompts for writing code

Gemini can also be a developer and help you with writing code in any programming language
of choice. As a developer this can help you speed up the process of writing code.
Imagine a folder on your machine with hundreds of files that needs to be renamed.
Renaming each file would take you a lot of time. You know a little Bash, and could write
a script to automate this, but that may also take awhile. So let’s write a prompt. You can
write the prompt in the public consumer chatbot Gemini, or if you are more concerned
about confidentiality, you can write these prompts within your Google Cloud account and
open Vertex Al Studio. The advantage of Vertex Al Studio is that you can configure the
temperature etc.

September 2024 42
<pre>
Prompt Engineering

Name 1 bash_rename files in folder
Goal Write a prompt to write code in Bash to rename files in a folder.
Model gemini-pro
Temperature 0.1 Token Limit 1024
Top-K N/A Top-P 1
Prompt Write a code snippet in Bash, which asks for a folder name.
Then it takes the contents of the folder and renames all the
files inside by prepending the name draft to the file name.
Output <code class="language-bash">
#!/bin/bash
# Ask for the folder name
echo "Enter the folder name: "
read folder_name
# Check if the folder exists
if [ ! -d "$folder_name" ]; then
    echo "Folder does not exist."
    exit 1
fi
# Get the list of files in the folder
# Loop through files in the specified folder
for file in "$folder_name"/*; do
    # Check if it's a file (not a directory)
    if [ -f "$file" ]; then
        # Construct the new file name within the target folder
        new_file_path="$folder_name/draft_$(basename "$file")"
        mv "$file" "$new_file_path"
    fi
done
echo "Files renamed successfully."
</code>
Table 16. Using Gemini to program code
</pre>
That seems like good code to me - it’s even documented! However, since LLMs can’t reason,
and repeat training data, it’s essential to read and test your code first.
The moment we are all waiting for, does it really work?

September 2024 43

Prompt Engineering

Let’s try it first with a test folder with only a few files in it, which will need to be renamed from
filename.txt to draft_filename.txt.
Copy the output from Table 16 (without the ‘bash ** text wrapper), and paste it in a new
file called: “rename_files.sh”.
Open a terminal window and type: . rename_files.sh. It will ask to enter a folder name, e.g.
test. and hit enter.
The script seems to run fine. You will see the message: Files renamed successfully.
When you look into the test folder, notice that all the files were perfectly renamed to
draft_filename.txt.
It worked!

Prompts for explaining code
As a developer when you work in teams you have to read someone else’s code. Gemini can
help you with this as well. Let’s take the code output from Table 16, remove the comments
and ask the large language model to explain what’s happening, See Table 17:

September 2024 44
<pre>
Prompt Engineering

Name 1_bash_explain_the_code
Goal Write a prompt to explain Bash code
Model gemini-pro
Temperature 0.1 Token Limit 1024
Top-K N/A Top-P 1
Prompt Explain to me the below Bash code:
<code class="language-bash">
#!/bin/bash
echo "Enter the folder name: "
read folder_name
if [ ! -d "$folder_name" ]; then
    echo "Folder does not exist."
    exit 1
fi
for file in "$folder_name"/*; do
    if [ -f "$file" ]; then
        new_file_path="$folder_name/draft_$(basename "$file")"
        mv "$file" "$new_file_path"
    fi
done
echo "Files renamed successfully."
</code>
Output This Bash script performs the following tasks:
1. User Input:
It prompts the user to enter a folder name using the ‘echo’ and ‘read’
commands. The entered folder name is stored in the ‘folder_name’ variable.
2. Folder Existence Check:
It checks if the folder with the specified name exists using the ‘[! -d
“$folder_name” ]’ condition. If the folder does not exist, it displays an error
message (“Folder does not exist.”) and exits the script with an exit code of 1.
Continues next page...
September 2024 45

Prompt Engineering

Output (Continued)
3. File Iteration and Renaming:
It iterates through each item in the specified folder using a ‘for’ loop ('for file in "$folder_name"/*').
Inside the loop, it first checks if the current item is a regular file (if [ -f "$file" ]).
If it is a file, it generates a new file name by prepending "draft_" to the original file name (using basename to get just the file name without the path). The new path is constructed to be within the original folder.
It then renames the file using the ‘mv “$file” “$new_file_path”’ command.
4. Success Message:
After processing all items in the folder, it displays a success message (“Files renamed
successfully.”) to indicate that the renaming operation was attempted for all files.
Table 17. A prompt for explaining code
</pre>
Prompts for translating code

The bash code from Table 16 seems to work fine. However this script could really be reusable
when it would prompt me for file names, ideally it should work as a separate application with
an UI. As a starting point, Python would be a better language for a (web) application than
Bash. LLMs can help with translating code from one language to another. See the example in
Table 18:

September 2024 46
<pre>
Prompt Engineering

Name 1_python_rename_files
Goal Write a prompt to translate Bash code to Python
Model gemini-pro
Temperature 0.1 Token Limit 1024
Top-K N/A Top-P 1
Prompt Translate the below Bash code to a Python snippet.
<code class="language-bash">
import os
import shutil
# Get the folder name from the user
folder_name = input("Enter the folder name: ")
# Check if the folder exists
if not os.path.isdir(folder_name):
    print("Folder does not exist.")
    exit(1)
# Iterate over the files in the directory
for filename in os.listdir(folder_name):
    original_file_path = os.path.join(folder_name, filename)
    # Check if it's a file (not a directory)
    if os.path.isfile(original_file_path):
        # Create a new file name with the prefix "draft_"
        new_filename_prefixed = f"draft_{filename}"
        new_file_path_with_prefix = os.path.join(folder_name, new_filename_prefixed)
        # Move/rename the file
        shutil.move(original_file_path, new_file_path_with_prefix)
# Print a success message
print("Files renamed successfully.")
</code>
Table 18. A prompt for translating code from Bash to Python
</pre>
Read and review the code. Copy the output from the prompt and paste it into a new file:
file_renamer.py. Test the code by opening a Terminal window, and execute the following
command python file_renamer.py.
NOTE: When prompting for (Python) code in the Language Studio in Vertex Al, you will have
to click on the ‘Markdown’ button. Otherwise you will receive plain text which is missing the
proper indenting of lines, which is important for running Python code.

September 2024 47`,

// Page 12: Code Prompting (Debugging/Reviewing)
`Prompt Engineering

Prompts for debugging and reviewing code

Let’s manually write some edits to the code of Table 18. It should prompt the user for the
filename prefix, and write this prefix in upper case characters. See the example code in
Snippet 3, but what a bummer. It now returns Python errors!

September 2024 48
<pre><code>
Prompt Engineering
Python
import os
import shutil
folder_name = input("Enter the folder name: ")
prefix = input("Enter the string to prepend to the filename: ")
text = prefix.upper() # Corrected line
# Check if the folder exists
if not os.path.isdir(folder_name):
    print("Folder does not exist.")
    exit(1)
# Iterate over the files
for filename in os.listdir(folder_name):
    original_file_path = os.path.join(folder_name, filename)
    if os.path.isfile(original_file_path):
        new_filename_with_prefix = f"{text}_{filename}"
        new_path_with_prefix = os.path.join(folder_name, new_filename_with_prefix)
        shutil.move(original_file_path, new_path_with_prefix)
# Print a success message
print("Files renamed successfully.")
</code>
Continues next page...
September 2024 50

Prompt Engineering

Output The code calls the ‘toUpperCase’ function to convert ‘prefix’ to uppercase, but
that function is not defined. To fix this issue, you can use the ‘upper() method of
the string class which converts a given string into uppercase. The modified code
is shown below:
<code class="language-python">
import os
import shutil
# Get the folder name from the user
folder_name = input("Enter the folder name: ")
prefix = input("Enter the string to prepend to the filename: ")
text = prefix.upper() # Corrected line
# Check if the folder exists
if not os.path.isdir(folder_name):
    print("Folder does not exist.")
    exit(1)
# Iterate over the files
for filename in os.listdir(folder_name):
    original_file_path = os.path.join(folder_name, filename)
    if os.path.isfile(original_file_path):
        new_filename_with_prefix = f"{text}_{filename}"
        new_path_with_prefix = os.path.join(folder_name, new_filename_with_prefix)
        shutil.move(original_file_path, new_path_with_prefix)
# Print a success message
print("Files renamed successfully.")
</code>
Continues next page...
September 2024 51

Prompt Engineering

Output (Continued)
Additionally, there are a few other improvements that can be made to the code:
* The file extension of the new filenames is not kept. It’s better to extract the
extension from the original filename and append it to the new filename.
* The code assumes that there are no spaces in the folder name. However,
it is a good practice to handle spaces gracefully by replacing them with
underscores or hyphens.
* Using the ‘f° string syntax for string interpolation is more readable and
concise than the traditional ‘+ operator.
* The code doesn’t handle errors that might occur during the renaming process.
It would be better to wrap the ‘shutil.move’ callin a ‘try...except’ block to
catch any potential errors.
Here is the improved code with these suggestions:
<code class="language-python">
import os
import shutil
# Get the folder name from the user
folder_name = input("Enter the folder name: ")
prefix_to_add = input("Enter the string to prepend to the filename: ")
# Check if the folder exists
if not os.path.isdir(folder_name):
    print("Folder does not exist.")
    exit(1)
# Iterate over the files
for current_filename in os.listdir(folder_name):
    original_file_path = os.path.join(folder_name, current_filename)
    if os.path.isfile(original_file_path):
        name_part, ext = os.path.splitext(current_filename)
        new_full_filename = f"{prefix_to_add.upper()}_{name_part}{ext}"
        new_full_path = os.path.join(folder_name, new_full_filename)
        try:
            shutil.move(original_file_path, new_full_path);
        except Exception as e:
        }
# Print a success message
print("Files renamed successfully.");
</code>
Table 19. A prompt for debugging and reviewing Python code
</pre>
That’s great. Not only did it tell me how to solve the problem, it also figured out that my code
had more bugs and how to solve them, too. The last part of the prompt gave suggestions to
improve the code in general.

September 2024 53`,

// Page 13: Multimodal & Best Practices (Intro)
`Prompt Engineering

What about multimodal prompting?

Prompting for code still uses the same regular large language model. Multimodal prompting
is a separate concern, it refers to a technique where you use multiple input formats to guide
a large language model, instead of just relying on text. This can include combinations of text,
images, audio, code, or even other formats, depending on the model’s capabilities and the
task at hand.

Best Practices

Finding the right prompt requires tinkering. Language Studio in Vertex Al is a perfect place to
play around with your prompts, with the ability to test against the various models.
Use the following best practices to become a pro in prompt engineering.

Provide examples

The most important best practice is to provide (one shot / few shot) examples within a
prompt. This is highly effective because it acts as a powerful teaching tool. These examples
showcase desired outputs or similar responses, allowing the model to learn from them
and tailor its own generation accordingly. It’s like giving the model a reference point or
target to aim for, improving the accuracy, style, and tone of its response to better match
your expectations.

September 2024 54

Prompt Engineering

Design with simplicity

Prompts should be concise, clear, and easy to understand for both you and the model. As a
rule of thumb, if it’s already confusing for you it will likely be also confusing for the model. Try
not to use complex language and don’t provide unnecessary information.
Examples:
BEFORE:
I am visiting New York right now, and I'd like to hear more about great
locations. I am with two 3 year old kids. Where should we go during
our vacation?
AFTER REWRITE:
Act as a travel guide for tourists. Describe great places to visit in
New York Manhattan with a 3 year old.

Try using verbs that describe the action. Here’s a set of examples:
Act, Analyze, Categorize, Classify, Contrast, Compare, Create, Describe, Define,
Evaluate, Extract, Find, Generate, Identify, List, Measure, Organize, Parse, Pick,
Predict, Provide, Rank, Recommend, Return, Retrieve, Rewrite, Select, Show, Sort,
Summarize, Translate, Write.

September 2024 55

Prompt Engineering

Be specific about the output

Be specific about the desired output. A concise instruction might not guide the LLM enough
or could be too generic. Providing specific details in the prompt (through system or context
prompting) can help the model to focus on what’s relevant, improving the overall accuracy.
Examples:
DO:
Generate a 3 paragraph blog post about the top 5 video game consoles.
The blog post should be informative and engaging, and it should be
written in a conversational style.
DO NOT:
Generate a blog post about video game consoles.

Use Instructions over Constraints

Instructions and constraints are used in prompting to guide the output of a LLM.
An instruction provides explicit instructions on the desired format, style, or content of the
response. It guides the model on what the model should do or produce.
A constraint is a set of limitations or boundaries on the response. It limits what the model
should not do or avoid.

September 2024 56`,

// Page 14: Best Practices (Continued)
`Prompt Engineering

Growing research suggests that focusing on positive instructions in prompting can be more
effective than relying heavily on constraints. This approach aligns with how humans prefer
positive instructions over lists of what not to do.
Instructions directly communicate the desired outcome, whereas constraints might leave the
model guessing about what is allowed. It gives flexibility and encourages creativity within the
defined boundaries, while constraints can limit the model’s potential. Also a list of constraints
can clash with each other.
Constraints are still valuable but in certain situations. To prevent the model from generating
harmful or biased content or when a strict output format or style is needed.
If possible, use positive instructions: instead of telling the model what not to do, tell it what to
do instead. This can avoid confusion and improve the accuracy of the output.
DO:
Generate a 1 paragraph blog post about the top 5 video game consoles.
Only discuss the console, the company who made it, the year, and total
sales.
DO NOT:
Generate a 1 paragraph blog post about the top 5 video game consoles.
Do not list video game names.

As a best practice, start by prioritizing instructions, clearly stating what you want the model
to do and only use constraints when necessary for safety, clarity or specific requirements.
Experiment and iterate to test different combinations of instructions and constraints to find
what works best for your specific tasks, and document these.

September 2024 57

Prompt Engineering

Control the max token length

To control the length of a generated LLM response, you can either set a max token limit in the
configuration or explicitly request a specific length in your prompt. For example:
"Explain quantum physics in a tweet length message."

Use variables in prompts

To reuse prompts and make it more dynamic use variables in the prompt, which can be
changed for different inputs. E.g. as shown in Table 20, a prompt which gives facts about a
city. Instead of hardcoding the city name in the prompt, use a variable. Variables can save
you time and effort by allowing you to avoid repeating yourself. If you need to use the same
piece of information in multiple prompts, you can store it in a variable and then reference
that variable in each prompt. This makes a lot of sense when integrating prompts into your
own applications.
<pre>
Prompt VARIABLES
{city} = "Amsterdam"
PROMPT
You are a travel guide. Tell me a fact about the city: {city}
Output Amsterdam is a beautiful city full of canals, bridges, and narrow streets. It’s a
great place to visit for its rich history, culture, and nightlife.
Table 20. Using variables in prompts
</pre>
September 2024 58

Prompt Engineering

Experiment with input formats and writing styles

Different models, model configurations, prompt formats, word choices, and submits can yield
different results. Therefore, it’s important to experiment with prompt attributes like the style,
the word choice, and the type prompt (zero shot, few shot, system prompt).
For example a prompt with the goal to generate text about the revolutionary video game
console Sega Dreamcast, can be formulated as a question, a statement or an instruction,
resulting in different outputs:
Question: What was the Sega Dreamcast and why was it such a revolutionary console?
Statement: The Sega Dreamcast was a sixth-generation video game console released by
Sega in 1999. It...
Instruction: Write a single paragraph that describes the Sega Dreamcast console and
explains why it was so revolutionary.

For few-shot prompting with classification tasks, mix up
the classes

Generally speaking, the order of your few-shots examples should not matter much. However,
when doing classification tasks, make sure you mix up the possible response classes in the
few shot examples. This is because you might otherwise be overfitting to the specific order of
the examples. By mixing up the possible response classes, you can ensure that the model is
learning to identify the key features of each class, rather than simply memorizing the order of
the examples. This will lead to more robust and generalizable performance on unseen data.

September 2024 59`,

// Page 15: Best Practices (Final) & Summary
`Prompt Engineering

A good rule of thumb is to start with 6 few shot examples and start testing the accuracy
from there.

Adapt to model updates

It’s important for you to stay on top of model architecture changes, added data, and
capabilities. Try out newer model versions and adjust your prompts to better leverage new
model features. Tools like Vertex Al Studio are great to store, test, and document the various
versions of your prompt.

Experiment with output formats

Besides the prompt input format, consider experimenting with the output format. For non-
creative tasks like extracting, selecting, parsing, ordering, ranking, or categorizing data try
having your output returned in a structured format like JSON or XML.
There are some benefits in returning JSON objects from a prompt that extracts data. In
a real-world application | don’t need to manually create this JSON format, | can already
return the data in a sorted order (very handy when working with datetime objects), but most
importantly, by prompting for a JSON format it forces the model to create a structure and
limit hallucinations.
Table 4 in the few-shot prompting section shows an example on how to return
structured output.

September 2024 60

Prompt Engineering

Experiment together with other prompt engineers

If you are in a situation where you have to try to come up with a good prompt, you might
want to find multiple people to make an attempt. When everyone follows the best practices
(as listed in this chapter) you are going to see a variance in performance between all the
different prompt attempts.

CoT Best practices
For Col prompting, putting the answer after the reasoning is required because the
generation of the reasoning changes the tokens that the model gets when it predicts the
final answer.
With CoT and self-consistency you need to be able to extract the final answer from your
prompt, separated from the reasoning.
For Col prompting, set the temperature to O.
Chain of thought prompting is based on greedy decoding, predicting the next word ina
sequence based on the highest probability assigned by the language model. Generally
speaking, when using reasoning, to come up with the final answer, there’s likely one single
correct answer. Therefore the temperature should always set to O.

September 2024 61

Prompt Engineering

Document the various prompt attempts

The last tip was mentioned before in this chapter, but we can’t stress enough how important
it is: document your prompt attempts in full detail so you can learn over time what went well
and what did not.
Prompt outputs can differ across models, across sampling settings, and even across different
versions of the same model. Moreover, even across identical prompts to the same model,
small differences in output sentence formatting and word choice can occur. (For example, as
mentioned previously, if two tokens have the same highest predicted probability, ties may be broken
randomly. This can then impact subsequent predicted tokens.).
We recommend creating a Google Sheet with Table 21 as a template. The advantages of
this approach are that you have a complete record when you inevitably have to revisit your
prompting work-either to pick it up in the future (you’d be surprised how much you can
forget after just a short break), to test prompt performance on different versions of a model,
and to help debug future errors.
Beyond the fields in this table, it’s also helpful to track the version of the prompt (iteration),
a field to capture if the result was OK/NOT OK/SOMETIMES OK, and a field to capture
feedback. If you’re lucky enough to be using Vertex Al Studio, save your prompts (using the
same name and version as listed in your documentation) and track the hyperlink to the saved
prompt in the table. This way, you’re always one click away from re-running your prompts.
When working on a retrieval augmented generation system, you should also capture the
specific aspects of the RAG system that impact what content was inserted into the prompt,
including the query, chunk settings, chunk output, and other information.

September 2024 62
<pre>
Prompt Engineering

Once you feel the prompt is close to perfect, take it to your project codebase. And in the
codebase, save prompts in a separate file from code, so it’s easier to maintain. Finally, ideally
your prompts are part of an operationalized system, and as a prompt engineer you should
rely on automated tests and evaluation procedures to understand how well your prompt
generalizes to a task.
Prompt engineering is an iterative process. Craft and test different prompts, analyze,
and document the results. Refine your prompt based on the model’s performance. Keep
experimenting until you achieve the desired output. When you change a model or model
configuration, go back and keep experimenting with the previously used prompts.

Name [name and version of your prompt]
Goal [One sentence explanation of the goal of this attempt]
Model [name and version of the used model]
Temperature [value between O - 1] Token Limit [number]
Top-K [number] Top-P [number]
Prompt [Write all the full prompt]
Output [Write out the output or multiple outputs]
Table 21. A template for documenting prompts
</pre>
Summary

This whitepaper discusses prompt engineering. We learned various prompting techniques,
such as:
* Zero prompting
* Few shot prompting
* System prompting
* Role prompting
* Contextual prompting
* Step-back prompting
* Chain of thought
* Self consistency
* Tree of thoughts
* ReAct

We even looked into ways how you can automate your prompts.
The whitepaper then discusses the challenges of gen Al like the problems that can happen
when your prompts are insufficient. We closed with best practices on how to become a better
prompt engineer.

September 2024 64`
        ];

        let currentPageIndex = 0;

        // DOM Elements for Reading Part
        const textDisplay = document.getElementById('text-container');
        const prevButton = document.getElementById('prev-page');
        const nextButton = document.getElementById('next-page');
        const doneReadingButtonContainer = document.getElementById('finish-reading-area');
        const doneReadingButton = document.getElementById('done-reading');
         const viewSamplingVisButton = document.getElementById('view-sampling-vis-button'); // Get the new button
        const confirmationPopup = document.getElementById('confirmation-popup');
        const proceedButton = document.getElementById('proceed-to-questions');
        const stayButton = document.getElementById('stay-on-page');
        const pageIndicator = document.getElementById('page-indicator');
        const readingArea = document.getElementById('reading-app-container');


        // --- Questions Part ---
        const allQuestions = [
    {
        question: "According to the Introduction, what is a text prompt in the context of a large language model?",
        options: [
            { text: "It's the input the model uses to predict a specific output.", value: "A" },
            { text: "It's the final output generated by the model.", value: "B" },
            { text: "It's a configuration setting like temperature.", value: "C" },
            { text: "It's the training data for the model.", value: "D" }
        ],
        correctAnswer: "A", // Updated for testing A
        explanation: "Think of a prompt as your instruction or starting point for the AI. The introduction clarifies that a text prompt is essentially the input you give the model so it knows what kind of output you're looking for."
    },
    {
        question: "Which LLM output configuration setting directly controls the degree of randomness in token selection?",
        options: [
            { text: "Top-K", value: "A" },
            { text: "Output length", value: "B" },
            { text: "Temperature", value: "C" },
            { text: "Top-P", value: "D" }
        ],
        correctAnswer: "C",
        explanation: "Temperature is the key here! As the material explains, adjusting the temperature setting allows you to influence how creative (random) or deterministic (predictable) the model's response will be when it's picking the next words."
    },
    {
        question: "What is 'zero-shot' prompting?",
        options: [
            { text: "Providing many examples to the model.", value: "A" },
            { text: "Providing a single example to the model.", value: "B" },
            { text: "A technique that always results in zero errors.", value: "C" },
            { text: "Providing only a task description and some starting text, with no examples.", value: "D" } // Updated option order for testing D
        ],
        correctAnswer: "D", // Updated for testing D
        explanation: "Zero-shot means you're asking the model to perform a task 'from scratch,' without showing it any examples first. The reading material points out that 'zero-shot' literally means 'no examples' are given in the prompt, just the task itself."
    },
    {
        question: "What is the main purpose of 'Role Prompting'?",
        options: [
            { text: "To assign a specific character or identity for the language model to adopt.", value: "A" }, // Updated for testing A
            { text: "To provide specific details or background information relevant to the current task.", value: "B" },
            { text: "To set the overall context and purpose for the language model.", value: "C" },
            { text: "To control the maximum number of tokens in the output.", value: "D" }
        ],
        correctAnswer: "A", // Updated for testing A
        explanation: "Role prompting is like giving the AI a character to play! The goal is to have it adopt a specific persona or identity, which helps ensure its responses are consistent in tone and style with that role."
    },
    {
        question: "Which prompting technique involves the LLM generating intermediate reasoning steps to improve accuracy?",
        options: [
            { text: "Chain of Thought (CoT) prompting", value: "A" }, // Updated for testing A
            { text: "Step-back prompting", value: "B" },
            { text: "Self-consistency", value: "C" },
            { text: "ReAct prompting", value: "D" }
        ],
        correctAnswer: "A", // Updated for testing A
        explanation: "Chain of Thought (CoT) prompting is the one that encourages the AI to 'think out loud' by generating the steps it takes to reach an answer. This step-by-step reasoning can lead to more accurate results."
    },
    {
        question: "What does 'Top-P' sampling control in LLM output configuration?",
        options: [
            { text: "The maximum number of tokens in the response.", value: "A" },
            { text: "The selection of the top K most likely tokens.", value: "B" },
            { text: "The temperature of the model's output.", value: "C" },
            { text: "The selection of top tokens whose cumulative probability does not exceed a certain value (P).", value: "D" } // Updated for testing D
        ],
        correctAnswer: "D", // Updated for testing D
        explanation: "Top-P (or nucleus sampling) is a clever way to control the pool of words the model considers. Instead of a fixed number (like Top-K), it picks from the most likely words until their combined probability reaches a set threshold 'P'."
    },
    {
        question: "What is a primary benefit of 'System Prompting' when requesting JSON output?",
        options: [
            { text: "It makes the output more creative.", value: "A" },
            { text: "It automatically translates the output to multiple languages.", value: "B" },
            { text: "It significantly reduces the computational cost.", value: "C" },
            { text: "It forces the model to create a structure and can limit hallucinations.", value: "D" } // Updated for testing D
        ],
        correctAnswer: "D", // Updated for testing D
        explanation: "When you use a system prompt to ask for JSON, you're giving the model a clear blueprint for its response. This helps keep it on track, create the desired structure, and reduce the chances of it making things up (hallucinating)."
    },
    {
        question: "What is 'Step-back' prompting designed to do?",
        options: [
            { text: "Have the LLM consider a general question first to activate relevant background knowledge.", value: "A" }, // Updated for testing A
            { text: "Make the LLM answer questions more quickly.", value: "B" },
            { text: "Force the LLM to only use its most recent training data.", value: "C" },
            { text: "Reduce the number of examples needed for few-shot prompting.", value: "D" }
        ],
        correctAnswer: "A", // Updated for testing A
        explanation: "Step-back prompting is like taking a moment to think about the bigger picture first. By prompting the AI with a more general, related question initially, it helps activate relevant knowledge before tackling the specific problem."
    },
    {
        question: "According to the 'Self-consistency' section, what does this technique combine to improve LLM reasoning?",
        options: [
            { text: "Greedy decoding and temperature control.", value: "A" },
            { text: "Sampling and majority voting.", value: "B" },
            { text: "Zero-shot prompting and role prompting.", value: "C" },
            { text: "Output length restriction and ReAct.", value: "D" }
        ],
        correctAnswer: "B",
        explanation: "Self-consistency is about getting multiple 'opinions' from the AI. It achieves this by using sampling to generate diverse reasoning paths and then picks the most common (majority vote) answer, often improving reliability."
    },
    {
        question: "What does ReAct prompting enable LLMs to do?",
        options: [
            { text: "Only reason about a problem without taking action.", value: "A" },
            { text: "Solve complex tasks using natural language reasoning combined with external tools.", value: "B" },
            { text: "Automatically generate new prompts.", value: "C" },
            { text: "Strictly follow a single linear chain of thought.", value: "D" }
        ],
        correctAnswer: "B",
        explanation: "ReAct (Reason and Act) makes LLMs more powerful by allowing them to not just think, but also to *do* things, like using external tools (e.g., a search engine or code interpreter) to gather information or perform actions."
    },
    {
        question: "What is the core idea behind Automatic Prompt Engineering (APE)?",
        options: [
            { text: "Manually crafting each prompt for maximum efficiency.", value: "A" },
            { text: "Using an LLM to generate more prompts, which are then evaluated.", value: "B" },
            { text: "Disabling all automatic features of the LLM.", value: "C" },
            { text: "Focusing solely on code generation prompts.", value: "D" }
        ],
        correctAnswer: "B",
        explanation: "APE is quite meta! It's about using an AI to help create better prompts for another AI. The process involves having one model generate various prompt candidates, which are then tested and refined."
    },
    {
        question: "When using Gemini for code generation, what is a key recommendation if you are concerned about confidentiality?",
        options: [
            { text: "Only use the public consumer chatbot Gemini.", value: "A" },
            { text: "Write prompts within your Google Cloud account using Vertex AI Studio.", value: "B" },
            { text: "Avoid code generation tasks altogether.", value: "C" },
            { text: "Set the temperature to maximum.", value: "D" }
        ],
        correctAnswer: "B",
        explanation: "For enhanced confidentiality with code generation, the material suggests using Vertex AI Studio within your own Google Cloud account. This offers a more controlled environment than a public chatbot."
    },
    {
        question: "What is one of the MOST important best practices in prompt engineering, according to the text?",
        options: [
            { text: "Always using the shortest possible prompts.", value: "A" },
            { text: "Providing (one-shot / few-shot) examples within a prompt.", value: "B" },
            { text: "Setting the temperature to 0 for all tasks.", value: "C" },
            { text: "Avoiding the use of variables in prompts.", value: "D" }
        ],
        correctAnswer: "B",
        explanation: "Showing the AI what you want is incredibly effective! The reading material emphasizes that providing examples (one-shot or few-shot) is a top best practice for guiding the model to produce the desired output."
    },
    {
        question: "When designing prompts, what does 'Design with simplicity' primarily mean?",
        options: [
            { text: "Making prompts concise, clear, and easy to understand.", value: "A" }, // Updated for testing A
            { text: "Using highly technical jargon to be precise.", value: "B" },
            { text: "Including as much information as possible, even if irrelevant.", value: "C" },
            { text: "Only using single-word prompts.", value: "D" }
        ],
        correctAnswer: "A", // Updated for testing A
        explanation: "Simplicity in prompt design is about clarity. Your prompts should be straightforward and easy for both you and the AI to understand, avoiding unnecessary complexity or confusing language."
    },
    {
        question: "Why is it generally better to use positive instructions over constraints in prompts?",
        options: [
            { text: "Constraints make the LLM more creative.", value: "A" },
            { text: "Positive instructions directly communicate the desired outcome, while constraints might leave the model guessing.", value: "B" },
            { text: "Constraints are never useful in prompting.", value: "C" },
            { text: "Positive instructions always lead to shorter LLM responses.", value: "D" }
        ],
        correctAnswer: "B",
        explanation: "Telling the AI *what to do* (positive instructions) is often more effective than just listing *what not to do* (constraints). Positive instructions clearly define the goal, giving the AI better direction."
    },
    {
        question: "For few-shot prompting with classification tasks, what is recommended regarding the order of examples?",
        options: [
            { text: "Always present examples of the same class consecutively.", value: "A" },
            { text: "The order does not matter at all.", value: "B" },
            { text: "Mix up the possible response classes in the few-shot examples.", value: "C" },
            { text: "Only use examples from the most common class.", value: "D" }
        ],
        correctAnswer: "C",
        explanation: "To prevent the model from just memorizing a pattern based on order, it's good practice to mix up the examples of different classes when doing few-shot prompting for classification tasks. This helps it learn the actual features of each class."
    },
    {
        question: "What is a key recommendation for CoT (Chain of Thought) prompting regarding temperature setting?",
        options: [
            { text: "Set the temperature to a high value for creativity.", value: "A" },
            { text: "Set the temperature to 0 (greedy decoding).", value: "B" },
            { text: "Temperature setting is irrelevant for CoT.", value: "C" },
            { text: "Alternate between high and low temperatures.", value: "D" }
        ],
        correctAnswer: "B",
        explanation: "For Chain of Thought, where you want the AI to follow a logical reasoning path, a temperature of 0 is generally recommended. This makes the AI choose the most probable next step, leading to more deterministic and focused reasoning."
    },
    {
        question: "What is the primary reason for documenting prompt attempts in detail?",
        options: [
            { text: "To make the prompt engineering process slower.", value: "A" },
            { text: "To have a record for revisiting work, testing on different model versions, and debugging.", value: "B" }, // Updated for testing B
            { text: "To share prompts publicly without context.", value: "C" },
            { text: "It's a requirement only for code-related prompts.", value: "D" }
        ],
        correctAnswer: "B", // Updated for testing B
        explanation: "Keeping good notes is crucial! Documenting your prompt experiments helps you learn what works, track changes, test with new model versions, and makes debugging much easier down the line."
    },
    {
        question: "What does 'Tree of Thoughts (ToT)' generalize from CoT prompting?",
        options: [
            { text: "It forces a single, linear path of reasoning.", value: "A" },
            { text: "It allows LLMs to explore multiple different reasoning paths simultaneously.", value: "B" },
            { text: "It eliminates the need for any intermediate steps.", value: "C" },
            { text: "It is only applicable to text summarization tasks.", value: "D" }
        ],
        correctAnswer: "B",
        explanation: "Tree of Thoughts takes Chain of Thought a step further. Instead of just one line of reasoning, ToT enables the AI to explore several different reasoning paths at once, like branches of a tree, which can be very useful for complex problems."
    },
    {
        question: "What is a disadvantage of using Chain of Thought (CoT) prompting mentioned in the text?",
        options:
                [
                    { text: "It reduces the interpretability of the model's response.", value: "A" },
                    { text: "It makes the model less robust across different LLM versions.", value: "B" },
                    { text: "The response includes reasoning, leading to more output tokens, higher cost, and longer time.", value: "C" },
                    { text: "It only works well with fine-tuned models.", value: "D" }
                ],
        correctAnswer: "C",
        explanation: "While CoT is great for reasoning, one downside is that the 'thinking steps' are included in the output. This means more words (tokens), which can make the response longer, take more time to generate, and potentially cost more."
    }
];

        // DOM Elements for Questions Part
        const questionsArea = document.getElementById('questions-area');
        const questionTextElement = document.getElementById('question-text');
        const optionsContainer = document.getElementById('options-container');
        const submitAnswerButton = document.getElementById('submit-answer');
        const nextQuestionButton = document.getElementById('next-question');
        const feedbackDiv = document.getElementById('feedback');
        const feedbackMessage = document.getElementById('feedback-message');
        const explanationDiv = document.getElementById('explanation');
        const illustrationImg = document.getElementById('illustration');
        const currentQuestionNumberSpan = document.getElementById('current-question-number');
        const totalQuestionsSpan = document.getElementById('total-questions');
        const backToReadingButton = document.getElementById('back-to-reading');
        const quizSummaryDiv = document.getElementById('quiz-summary');

        // DOM Elements for Strategies Review Menu
        const strategyReviewMenu = document.getElementById('strategy-review-menu');
        const strategiesList = document.getElementById('strategies-list');
        const backToSummaryButton = document.getElementById('back-to-summary');

        // DOM Elements for Sampling Visualization
        const samplingVisualizationContainer = document.getElementById('sampling-visualization-container');
        const tempSlider = document.getElementById('temp-slider');
        const tempNumber = document.getElementById('temp-number');
        const topkSlider = document.getElementById('topk-slider');
        const topkNumber = document.getElementById('topk-number');
        const toppSlider = document.getElementById('topp-slider');
        const toppNumber = document.getElementById('topp-number');
        const samplingExplanation = document.getElementById('sampling-explanation');
        const backToReadingVisButton = document.getElementById('back-to-reading-vis');


        // Quiz state variables
        let shuffledQuestionIndices = [];
        let currentQuestionIndexInShuffledList = 0;
        let correctAnswersCount = 0;
        let isRedoMode = false;
        let incorrectQuestionIndices = []; // Will store original indices of wrong questions
        
        // Strategy Summary Data (Based on the article's Summary section)
         const promptingStrategiesSummary = [
             { name: "Zero-Shot Prompting", summary: "This is the most basic way to prompt an AI. You simply give it a task or ask a question without providing any examples of how you want the output to look. The AI relies entirely on its training data to figure out how to respond." },
             { name: "One-Shot & Few-Shot Prompting", summary: "With these techniques, you include one (one-shot) or a few (few-shot) examples directly in your prompt before asking the actual question. These examples show the AI the desired format, style, or type of output you expect, helping it follow a specific pattern." },
             { name: "System Prompting", summary: "You give the AI high-level instructions that set its overall goal or behavior for a task or conversation. This defines the main job the AI should be doing, like always responding in a specific format (e.g., JSON) or following certain rules." },
             { name: "Role Prompting", summary: "This involves telling the AI to act like a particular person or character (like a doctor, teacher, or comedian). The AI will then try to generate responses using the tone, style, and knowledge that character would typically have. This helps frame the AI's output personality." },
             { name: "Contextual Prompting", summary: "This is about providing the AI with specific background information or details that are important for the current task or question. This context helps the AI understand the specific situation and tailor its answer more accurately, especially in ongoing interactions." },
             { name: "Step-back Prompting", summary: "Instead of asking a difficult specific question directly, you first prompt the AI with a simpler, more general question related to the topic. The AI answers the general question, and you then provide both the original specific problem and the AI's general answer as context for a second prompt, which helps the AI solve the specific problem better." },
             { name: "Chain of Thought (CoT)", summary: "This technique encourages the AI to show its step-by-step thinking process before arriving at the final answer. By adding phrases like 'Let's think step by step' to your prompt, you guide the AI to generate intermediate reasoning steps. This improves accuracy for complex tasks and makes the AI's logic more understandable." },
             { name: "Self Consistency", summary: "This technique involves prompting the AI for the same question multiple times, often using different settings to get diverse reasoning paths and answers. You then compare the final answers from all attempts and choose the one that appears most often (a 'majority vote'). This 'consensus' approach helps improve the reliability and accuracy of the final result, especially for tasks that can be tricky for the AI." },
             { name: "Tree of Thoughts (ToT)", summary: "This is an advanced reasoning technique that builds on Chain of Thought. Instead of following just one possible linear path of reasoning, ToT allows the AI to explore and manage multiple different reasoning paths simultaneously, much like the branches of a tree. This is particularly powerful for tackling very complex problems that might have several possible approaches to consider." },
             { name: "ReAct (Reason & Act)", summary: "This technique enables the AI to combine thinking (reasoning) with the ability to interact with and use external tools like web search engines, calculators, or code interpreters. The AI reasons about what is needed, decides what action to take with a tool, processes the tool's output, and then uses that result to continue reasoning and work towards the final solution. It mimics how humans might use tools and internal thought to solve problems." },
             { name: "Automatic Prompt Engineering (APE)", summary: "This is a method to automate the process of designing effective prompts for an AI model. You use an AI model itself to generate many different versions of prompts for a specific task, and then use automated evaluation methods to automatically test and find the best-performing prompt. This significantly reduces the need for humans to manually craft and experiment with prompts." }
         ];


        // --- Reading Part Functions ---
        function displayPage() {
            if (pages.length === 0) {
                textDisplay.innerHTML = "<p>No content available.</p>";
                prevButton.style.display = 'none';
                nextButton.style.display = 'none';
                doneReadingButtonContainer.style.display = 'none';
                pageIndicator.textContent = "Page 0 of 0";
                return;
            }
            textDisplay.innerHTML = pages[currentPageIndex];
            pageIndicator.textContent = `Page ${currentPageIndex + 1} of ${pages.length}`;
            window.scrollTo(0, 0);

            prevButton.disabled = currentPageIndex === 0;
            nextButton.style.display = currentPageIndex === pages.length - 1 ? 'none' : 'inline-block';

            // Only show the finish reading button container on the last page
            doneReadingButtonContainer.style.display = currentPageIndex === pages.length - 1 ? 'flex' : 'none'; // Use flex for button container
            // Hide the visualization button by default unless on the last page
             viewSamplingVisButton.style.display = currentPageIndex === pages.length - 1 ? 'inline-block' : 'none';

        }

        // --- Quiz Functions ---
        function shuffleArray(array) {
            for (let i = array.length - 1; i > 0; i--) {
                const j = Math.floor(Math.random() * (i + 1));
                [array[i], array[j]] = [array[j], array[i]];
            }
            return array;
        }

        function startQuiz() {
            isRedoMode = false;
            incorrectQuestionIndices = [];

            confirmationPopup.style.display = 'none';
            correctAnswersCount = 0; // Reset score
            quizSummaryDiv.style.display = 'none'; // Hide summary from previous attempts
            quizSummaryDiv.innerHTML = '';
             strategyReviewMenu.style.display = 'none'; // Hide strategies menu
             samplingVisualizationContainer.style.display = 'none'; // Hide visualization


            if (allQuestions.length === 0) {
                readingArea.style.display = 'none';
                doneReadingButtonContainer.style.display = 'none';
                questionsArea.style.display = 'block';
                // Hide main question UI elements if no questions
                document.querySelector('#questions-area h2').style.display = 'none';
                questionTextElement.style.display = 'none';
                optionsContainer.style.display = 'none';
                submitAnswerButton.style.display = 'none';
                quizSummaryDiv.innerHTML = `<h2>No questions available at this time.</h2>`;
                quizSummaryDiv.style.display = 'block';
                return;
            }
            
            // Ensure main question UI elements are visible for a new quiz
            document.querySelector('#questions-area h2').style.display = 'block';
            questionTextElement.style.display = 'block';
            optionsContainer.style.display = 'block';
            submitAnswerButton.style.display = 'inline-block'; // Show submit button

            readingArea.style.display = 'none';
            doneReadingButtonContainer.style.display = 'none'; // Hide reading buttons when in quiz
            questionsArea.style.display = 'block';


            shuffledQuestionIndices = Array.from({ length: allQuestions.length }, (_, i) => i);
            shuffleArray(shuffledQuestionIndices);
            currentQuestionIndexInShuffledList = 0;

            totalQuestionsSpan.textContent = allQuestions.length;
            displaySingleQuestion();
        }

                // Inside your existing <script> tag

                // Inside your existing <script> tag

                    function displaySingleQuestion() {
            // --- Start of new animation code ---
            // Apply fade-in to the main question elements container
            // Ensure these elements exist and are the ones you want to animate as a group
            const questionContentGroup = document.getElementById('question-text-container'); // Or a common parent
            const optionsGroup = document.getElementById('options-container');

            // Remove previous animation classes
            questionContentGroup.classList.remove('question-transition-in', 'question-transition-out');
            optionsGroup.classList.remove('question-transition-in', 'question-transition-out');
            
            // Force reflow to restart animation if classes were just removed and re-added
            void questionContentGroup.offsetWidth;
            void optionsGroup.offsetWidth;

            // Apply fade-in
            questionContentGroup.classList.add('question-transition-in');
            optionsGroup.classList.add('question-transition-in');
            // --- End of new animation code ---


            quizSummaryDiv.style.display = 'none';
            strategyReviewMenu.style.display = 'none'; // Ensure strategies menu is hidden
            samplingVisualizationContainer.style.display = 'none'; // Ensure visualization is hidden
            questionTextElement.style.display = 'block'; // Ensure these are block for animation
            optionsContainer.style.display = 'block'; // Ensure these are block for animation


            const currentQuestionDataIndex = shuffledQuestionIndices[currentQuestionIndexInShuffledList];
            const currentQuestion = allQuestions[currentQuestionDataIndex];

            currentQuestionNumberSpan.textContent = currentQuestionIndexInShuffledList + 1;
            questionTextElement.textContent = currentQuestion.question;

            optionsContainer.innerHTML = ''; // Clear old options *after* fade out ideally, but this is simpler
            feedbackDiv.style.display = 'none';
            feedbackMessage.textContent = '';
            explanationDiv.innerHTML = '';
            illustrationImg.style.display = 'none';
            illustrationImg.src = '';


            currentQuestion.options.forEach(option => {
                const optionId = `option-${option.value}`;
                const label = document.createElement('label');
                label.setAttribute('for', optionId);
                
                const radio = document.createElement('input');
                radio.type = 'radio';
                radio.id = optionId;
                radio.name = 'answer';
                radio.value = option.value;
                
                label.appendChild(radio);
                label.appendChild(document.createTextNode(` ${option.text}`));

                optionsContainer.appendChild(label);
            });

            submitAnswerButton.style.display = 'inline-block';
            nextQuestionButton.style.display = 'none';
            optionsContainer.querySelectorAll('input[type="radio"]').forEach(radio => radio.disabled = false);
            optionsContainer.querySelectorAll('label').forEach(label => {
                label.classList.remove('correct-answer', 'wrong-answer');
            });
        }

                // Inside your existing <script> tag

                    function handleSubmitAnswer() {
            const selectedOptionInput = document.querySelector('input[name="answer"]:checked');

            if (!selectedOptionInput) {
                alert("Please select an answer.");
                return;
            }

            // ... (rest of your existing logic to get userAnswer, currentQuestion, correctAnswerValue) ...
            const userAnswer = selectedOptionInput.value;
            const currentQuestionDataIndex = shuffledQuestionIndices[currentQuestionIndexInShuffledList];
            const currentQuestion = allQuestions[currentQuestionDataIndex];
            const correctAnswerValue = currentQuestion.correctAnswer;

            optionsContainer.querySelectorAll('input[type="radio"]').forEach(radio => radio.disabled = true);

            const correctLabel = optionsContainer.querySelector(`label[for="option-${correctAnswerValue}"]`);
            if (correctLabel) {
                correctLabel.classList.add('correct-answer');
            }

            // --- Modify feedback div classes ---
            feedbackDiv.classList.remove('correct-answer-feedback', 'wrong-answer-feedback', 'question-transition-in');
            void feedbackDiv.offsetWidth; // Reflow

            if (userAnswer === correctAnswerValue) {
                feedbackMessage.textContent = "Correct!";
                feedbackMessage.style.color = '#0f5132'; // Darker green for text
                explanationDiv.innerHTML = currentQuestion.explanation ? `<p>${currentQuestion.explanation}</p>` : '';
                correctAnswersCount++;
                feedbackDiv.classList.add('correct-answer-feedback');
            } else {
                feedbackMessage.textContent = "Wrong!";
                feedbackMessage.style.color = '#721c24'; // Darker red for text
                explanationDiv.innerHTML = `<p>The correct answer was ${correctAnswerValue}.</p>${currentQuestion.explanation ? `<p>${currentQuestion.explanation}</p>` : ''}`;
                incorrectQuestionIndices.push(currentQuestionDataIndex);

                const selectedLabel = selectedOptionInput.closest('label');
                if (!isRedoMode || (isRedoMode && !allQuestions[currentQuestionDataIndex].justCorrectedInRedo)) 
                {
                    incorrectQuestionIndices.push(currentQuestionDataIndex);
                }
                
            }
            
            if (currentQuestion.illustration) {
                illustrationImg.src = currentQuestion.illustration;
                illustrationImg.style.display = 'block';
            } else {
                illustrationImg.style.display = 'none';
            }

            feedbackDiv.style.display = 'block';
            feedbackDiv.classList.add('question-transition-in'); // Fade in feedback
            // --- End of feedback div class modification ---

            submitAnswerButton.style.display = 'none';
            nextQuestionButton.style.display = 'inline-block';
        }
                // Inside your existing <script> tag


        // Add this function inside your <script> tag

            function getGermanGrade(percentage) {
            if (percentage >= 92) return { grade: "1,0", description: "Sehr gut" }; // (Very Good - Excellent)
            if (percentage >= 87) return { grade: "1,3", description: "Sehr gut" };
            if (percentage >= 82) return { grade: "1,7", description: "Gut" };       // (Good)
            if (percentage >= 77) return { grade: "2,0", description: "Gut" };
            if (percentage >= 72) return { grade: "2,3", description: "Gut" };
            if (percentage >= 67) return { grade: "2,7", description: "Befriedigend" }; // (Satisfactory)
            if (percentage >= 62) return { grade: "3,0", description: "Befriedigend" };
            if (percentage >= 57) return { grade: "3,3", description: "Befriedigend" };
            if (percentage >= 50) return { grade: "3,7", description: "Ausreichend" }; // (Sufficient)
            if (percentage >= 45) return { grade: "4,0", description: "Ausreichend" }; // Technically passing
            // Below 4.0 is typically "Mangelhaft" (Deficient) or "Ungenügend" (Insufficient)
            if (percentage >= 30) return { grade: "5,0", description: "Mangelhaft" }; // (Deficient - Fail)
            return { grade: "6,0", description: "Ungenügend" };                   // (Insufficient - Fail)
        }



        
        
        
        
        
        
        
                // REPLACE your existing handleNextQuestion function with this one
                function handleNextQuestion() {
            const questionContentGroup = document.getElementById('question-text-container');
            const optionsGroup = document.getElementById('options-container');
            const feedbackGroup = document.getElementById('feedback');

            questionContentGroup.classList.remove('question-transition-in');
            optionsGroup.classList.remove('question-transition-in');
            feedbackGroup.classList.remove('question-transition-in');

            questionContentGroup.classList.add('question-transition-out');
            optionsGroup.classList.add('question-transition-out');
            feedbackGroup.classList.add('question-transition-out');

            setTimeout(() => {
                currentQuestionIndexInShuffledList++;
                if (currentQuestionIndexInShuffledList < shuffledQuestionIndices.length) {
                    displaySingleQuestion();
                } else { // Quiz or Redo session finished
                    questionTextElement.style.display = 'none';
                    optionsContainer.style.display = 'none';
                    submitAnswerButton.style.display = 'none';
                    nextQuestionButton.style.display = 'none';
                    feedbackDiv.style.display = 'none';
                    document.querySelector('#questions-area h2').style.display = 'none';

                    const totalInThisSession = shuffledQuestionIndices.length;
                    const percentage = totalInThisSession > 0 ? Math.round((correctAnswersCount / totalInThisSession) * 100) : 0;

                    let summaryHTML = ``;
                    if (isRedoMode) {
                        summaryHTML += `<h2>Wiederholung Falscher Fragen - Auswertung!</h2>`;
                        summaryHTML += `<p>Du hast ${correctAnswersCount} von ${totalInThisSession} zuvor falsch beantworteten Fragen jetzt richtig (${percentage}%).</p>`;
                        if (incorrectQuestionIndices.length > 0) { // These are newly incorrect from the redo session
                             summaryHTML += `<p>Es gibt immer noch ${incorrectQuestionIndices.length} Fragen, die du falsch beantwortet hast. Du kannst sie erneut wiederholen.</p>`;
                        } else if (totalInThisSession > 0) { // All redone questions were answered correctly
                            summaryHTML += `<p>Sehr gut! Alle zuvor falschen Fragen wurden jetzt richtig beantwortet!</p>`;
                        } else { // Should not happen if startRedoIncorrect checks length
                            summaryHTML += `<p>Keine Fragen in dieser Wiederholungsrunde.</p>`;
                        }
                    } else { // Original quiz summary
                        const germanGradeInfo = getGermanGrade(percentage);
                        summaryHTML += `<h2>Quiz Auswertung!</h2>`;
                        summaryHTML += `<p>Du hast ${correctAnswersCount} von ${totalInThisSession} Fragen richtig beantwortet (${percentage}%).</p>`;
                        summaryHTML += `<p>Deine Note: <strong>${germanGradeInfo.grade} (${germanGradeInfo.description})</strong></p>`;
                        if (percentage >= 82) summaryHTML += `<p>Sehr gute Leistung! Du hast ein starkes Verständnis des Materials.</p>`;
                        else if (percentage >= 62) summaryHTML += `<p>Gute Leistung! Einige Bereiche könntest du für ein tieferes Verständnis wiederholen.</p>`;
                        else if (percentage >= 45) summaryHTML += `<p>Bestanden! Es gibt jedoch noch Raum für Verbesserungen.</p>`;
                        else summaryHTML += `<p>Übe weiter! Das Wiederholen des Materials und ein erneuter Versuch helfen dir, dich zu verbessern.</p>`;
                    }

                    quizSummaryDiv.innerHTML = summaryHTML;
                    quizSummaryDiv.style.display = 'block';
                    quizSummaryDiv.classList.add('question-transition-in');
                    backToReadingButton.style.display = 'inline-block';

                    // Add "Redo Incorrect Questions" button if there are any NEWLY incorrect questions from THIS session
                    // or if it was the main quiz and there were incorrects.
                    if (incorrectQuestionIndices.length > 0) {
                        // Check if a redo button already exists to avoid duplicates if logic gets complex
                        if (!document.getElementById('redo-incorrect-button')) {
                            const redoButton = document.createElement('button');
                            redoButton.id = 'redo-incorrect-button';
                            redoButton.textContent = isRedoMode ? 'Erneut falsche Fragen wiederholen' : 'Falsche Fragen wiederholen';
                            redoButton.style.marginLeft = '10px';
                            quizSummaryDiv.appendChild(redoButton);
                            redoButton.addEventListener('click', startRedoIncorrect);
                        }
                    }
                    // Add Review Strategies button after summary
                    const reviewStrategiesButton = document.createElement('button');
                    reviewStrategiesButton.id = 'review-strategies-button';
                    reviewStrategiesButton.textContent = 'Review Strategies Summary';
                    reviewStrategiesButton.style.marginLeft = '10px';
                    quizSummaryDiv.appendChild(reviewStrategiesButton);
                    reviewStrategiesButton.addEventListener('click', showStrategyReviewMenu);


                    isRedoMode = false; // Reset after displaying redo summary if it was a redo session.
                                      // If it was a main quiz, this has no effect until startRedoIncorrect is called.
                } // This was the correctly placed brace from the previous fix

                questionContentGroup.classList.remove('question-transition-out');
                optionsGroup.classList.remove('question-transition-out');
                feedbackGroup.classList.remove('question-transition-out');
            }, 300);
        }


                // Add this ENTIRE function as a new function in your script

        function startRedoIncorrect() {
            if (incorrectQuestionIndices.length === 0) {
                alert("Du hattest keine falschen Antworten zum Wiederholen!");
                return;
            }

            isRedoMode = true;
            shuffledQuestionIndices = [...incorrectQuestionIndices]; // Use the list of incorrect indices
            incorrectQuestionIndices = []; // Clear the list for the current redo session (to track new mistakes in this redo)
            
            correctAnswersCount = 0;
            currentQuestionIndexInShuffledList = 0;

            quizSummaryDiv.style.display = 'none';
            quizSummaryDiv.classList.remove('question-transition-in');
            strategyReviewMenu.style.display = 'none'; // Hide strategies menu
            samplingVisualizationContainer.style.display = 'none'; // Hide visualization


            document.querySelector('#questions-area h2').style.display = 'block';
            totalQuestionsSpan.textContent = shuffledQuestionIndices.length;

            // Make sure question content area is visible again
            document.getElementById('question-text-container').style.display = 'block';
            document.getElementById('options-container').style.display = 'block';
            document.getElementById('submit-answer').style.display = 'inline-block';
            document.getElementById('next-question').style.display = 'none';


            displaySingleQuestion();
        }

        // New function to show the strategies review menu
        function showStrategyReviewMenu() {
            questionsArea.style.display = 'none';
            quizSummaryDiv.style.display = 'none'; // Hide summary
             samplingVisualizationContainer.style.display = 'none'; // Hide visualization

            strategyReviewMenu.style.display = 'block'; // Show the strategies menu

            strategiesList.innerHTML = ''; // Clear previous list items

            promptingStrategiesSummary.forEach(strategy => {
                const listItem = document.createElement('li');
                listItem.innerHTML = `<strong>${strategy.name}:</strong> ${strategy.summary}`;
                strategiesList.appendChild(listItem);
            });
        }

        // New function to go back to the quiz summary from the strategies menu
        function backToQuizSummary() {
             strategyReviewMenu.style.display = 'none'; // Hide strategies menu
             samplingVisualizationContainer.style.display = 'none'; // Hide visualization

             questionsArea.style.display = 'block'; // Show questions area (which contains the summary)
             quizSummaryDiv.style.display = 'block'; // Show the summary itself
        }

         // New function to show the sampling visualization menu
         function showSamplingVisualization() {
             readingArea.style.display = 'none'; // Hide reading area
             questionsArea.style.display = 'none'; // Hide questions area
             strategyReviewMenu.style.display = 'none'; // Hide strategies menu

             samplingVisualizationContainer.style.display = 'block'; // Show visualization container

             // Set initial values and update explanation
             updateSamplingVisualization();
         }

         // New function to update the sampling visualization explanation
         function updateSamplingVisualization() {
             const temp = parseFloat(tempSlider.value).toFixed(2);
             const topk = parseInt(topkSlider.value);
             const topp = parseFloat(toppSlider.value).toFixed(2);

             tempNumber.value = temp;
             topkNumber.value = topk;
             toppNumber.value = topp;

             let explanation = `With Temperature set to <strong>${temp}</strong>, Top-K to <strong>${topk}</strong>, and Top-P to <strong>${topp}</strong>:<br><br>`;

             if (temp == 0) {
                 explanation += `<strong>Temperature (0):</strong> The model will always choose the single token with the highest predicted probability. This results in highly deterministic and predictable output. Top-K and Top-P settings become irrelevant at this temperature.`;
             } else if (temp >= 1.5) { // High temperature
                 explanation += `<strong>Temperature (${temp}):</strong> With a high temperature, the model makes more random choices. The probabilities of less likely tokens are increased, leading to highly diverse, creative, and potentially less coherent output. The effect of Top-K and Top-P is lessened as more tokens become almost equally likely.<br>`;
                  if (topk < 100) { // Assuming 100 is max K for this example
                      explanation += `<strong>Top-K (${topk}):</strong> Only the top ${topk} most probable tokens are considered before temperature is applied.<br>`;
                  } else {
                      explanation += `<strong>Top-K (High):</strong> Almost all tokens are considered.<br>`;
                  }
                   if (topp < 1) {
                      explanation += `<strong>Top-P (${topp}):</strong> Tokens are selected from the smallest set whose cumulative probability exceeds ${topp} before temperature is applied.<br>`;
                  } else {
                      explanation += `<strong>Top-P (1.0):</strong> All tokens are considered.<br>`;
                   }

             }
             else { // Moderate temperature
                  explanation += `<strong>Temperature (${temp}):</strong> This setting introduces some randomness, making the output less predictable than Temperature 0, but generally more focused than very high temperatures. It influences how the probabilities of the selected tokens are considered.<br>`;
                  if (topk < 100) {
                       explanation += `<strong>Top-K (${topk}):</strong> The model first filters to consider only the top ${topk} most probable tokens.<br>`;
                   } else {
                       explanation += `<strong>Top-K (High):</strong> Almost all tokens are considered before other sampling is applied.<br>`;
                   }
                    if (topp < 1) {
                       explanation += `<strong>Top-P (${topp}):</strong> Then, from the remaining tokens (if Top-K was applied), tokens are selected from the smallest set whose cumulative probability exceeds ${topp}. Temperature is applied after these steps.<br>`;
                   } else {
                       explanation += `<strong>Top-P (1.0):</strong> If Top-K was applied, all tokens within the top K are considered before temperature. If Top-K was not limiting, all tokens with >0 probability are considered before temperature.<br>`;
                    }
             }

             if (topk == 1 && temp != 0) {
                  explanation += `<strong>Note:</strong> When Top-K is 1 and Temperature is not 0, the result is similar to Temperature 0 – only the single most probable token (which is the only one in the Top-K set) is effectively chosen, making the output deterministic within that single token choice.`;
             } else if (topp == 0 && temp != 0) { // Assuming Top-P 0 means only the single highest prob token
                  explanation += `<strong>Note:</strong> When Top-P is 0 (or very low) and Temperature is not 0, this often means only the single most probable token is considered, similar to Temperature 0.`;
             }


             samplingExplanation.innerHTML = explanation;
         }

         // New function to go back from visualization to reading
         function backFromVisualization() {
             samplingVisualizationContainer.style.display = 'none'; // Hide visualization
             readingArea.style.display = 'block'; // Show reading area
             // Ensure the last page is displayed when returning
             displayPage();
         }


        function backToReading() {
            questionsArea.style.display = 'none';
            readingArea.style.display = 'block';
            quizSummaryDiv.style.display = 'none'; // Hide summary
            strategyReviewMenu.style.display = 'none'; // Hide strategies menu
            samplingVisualizationContainer.style.display = 'none'; // Hide visualization

            // Restore visibility of question elements for next quiz attempt
            document.querySelector('#questions-area h2').style.display = 'block';
            questionTextElement.style.display = 'block';
            optionsContainer.style.display = 'block';
            submitAnswerButton.style.display = 'inline-block'; // Ready for new quiz start

            currentPageIndex = 0;
            displayPage();
        }


        // --- Event Listeners ---
        prevButton.addEventListener('click', () => {
            if (currentPageIndex > 0) {
                currentPageIndex--;
                displayPage();
            }
        });

        nextButton.addEventListener('click', () => {
            if (currentPageIndex < pages.length - 1) {
                currentPageIndex++;
                displayPage();
            }
        });

        doneReadingButton.addEventListener('click', () => {
            confirmationPopup.style.display = 'block';
        });

        proceedButton.addEventListener('click', startQuiz);

        stayButton.addEventListener('click', () => {
            confirmationPopup.style.display = 'none';
        });

        submitAnswerButton.addEventListener('click', handleSubmitAnswer);
        nextQuestionButton.addEventListener('click', handleNextQuestion);
        backToReadingButton.addEventListener('click', backToReading);

        // Event listener for the "Back to Summary" button in the strategies menu
        backToSummaryButton.addEventListener('click', backToQuizSummary);

        // Event listener for the new "View Sampling Visualization" button
         viewSamplingVisButton.addEventListener('click', showSamplingVisualization);

        // Event listeners for the sliders and number inputs to update the visualization explanation
        tempSlider.addEventListener('input', updateSamplingVisualization);
        tempNumber.addEventListener('input', function() {
             tempSlider.value = this.value; // Sync slider to number input
             updateSamplingVisualization();
        });

        topkSlider.addEventListener('input', updateSamplingVisualization);
         topkNumber.addEventListener('input', function() {
             topkSlider.value = this.value; // Sync slider to number input
             updateSamplingVisualization();
         });


        toppSlider.addEventListener('input', updateSamplingVisualization);
         toppNumber.addEventListener('input', function() {
             toppSlider.value = this.value; // Sync slider to number input
             updateSamplingVisualization();
         });


        // Event listener for the "Back to Reading" button in the visualization menu
         backToReadingVisButton.addEventListener('click', backFromVisualization);


        // Initial page load for reading material
        displayPage();
    
    </script>
</body>
</html>